{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyun-Joon Yang\n",
    "# yanghyun@usc.edu\n",
    "# BISC 444\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
    "#     install.packages(\"BiocManager\")\n",
    "\n",
    "# BiocManager::install(\"DNAshapeR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages('caret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: GenomicRanges\n",
      "Loading required package: stats4\n",
      "Loading required package: BiocGenerics\n",
      "Loading required package: parallel\n",
      "\n",
      "Attaching package: 'BiocGenerics'\n",
      "\n",
      "The following objects are masked from 'package:parallel':\n",
      "\n",
      "    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n",
      "    clusterExport, clusterMap, parApply, parCapply, parLapply,\n",
      "    parLapplyLB, parRapply, parSapply, parSapplyLB\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    anyDuplicated, append, as.data.frame, basename, cbind, colnames,\n",
      "    dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n",
      "    grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\n",
      "    order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n",
      "    rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\n",
      "    union, unique, unsplit, which, which.max, which.min\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "Warning message:\n",
      "\"package 'S4Vectors' was built under R version 3.6.3\"\n",
      "Attaching package: 'S4Vectors'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    expand.grid\n",
      "\n",
      "Loading required package: IRanges\n",
      "Warning message:\n",
      "\"package 'IRanges' was built under R version 3.6.2\"\n",
      "Attaching package: 'IRanges'\n",
      "\n",
      "The following object is masked from 'package:grDevices':\n",
      "\n",
      "    windows\n",
      "\n",
      "Loading required package: GenomeInfoDb\n",
      "Warning message:\n",
      "\"package 'GenomeInfoDb' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'caret' was built under R version 3.6.3\"Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    }
   ],
   "source": [
    "library(DNAshapeR)\n",
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Predict shape from sequence\n",
    "\n",
    "DNAshapeR package was used to predict DNA shape for each sequence bound by transcription factor Mad, Max and Myc. The datasets were obtained from the in vitro gcPBM (genomic context protein binding microarray) assay. The sequence data in FASTA format (Mad.fa, Max.fa and Myc.fa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names\n",
    "ns <- c('Mad', 'Max', 'Myc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview of the raw data\n",
    "For each of the TF's FASTA file is just a single column in which the rows alternate by comment 'seq#' and the actual sequence that is 36 nucleotides long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mad"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq1                               </span></td></tr>\n",
       "\t<tr><td>GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq2                               </span></td></tr>\n",
       "\t<tr><td>CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq3                               </span></td></tr>\n",
       "\t<tr><td>GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " V1\\\\\n",
       "\\hline\n",
       "\t >seq1                               \\\\\n",
       "\t GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG\\\\\n",
       "\t >seq2                               \\\\\n",
       "\t CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG\\\\\n",
       "\t >seq3                               \\\\\n",
       "\t GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| V1 |\n",
       "|---|\n",
       "| >seq1                                |\n",
       "| GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG |\n",
       "| >seq2                                |\n",
       "| CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG |\n",
       "| >seq3                                |\n",
       "| GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT |\n",
       "\n"
      ],
      "text/plain": [
       "  V1                                  \n",
       "1 >seq1                               \n",
       "2 GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG\n",
       "3 >seq2                               \n",
       "4 CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG\n",
       "5 >seq3                               \n",
       "6 GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>15068</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 15068\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 15068\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 15068     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq1                               </span></td></tr>\n",
       "\t<tr><td>GAAGCCCTGGCGGGGCGCGTGCCCGCCGCCGCCGCC</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq2                               </span></td></tr>\n",
       "\t<tr><td>TCCCGGGGCTAGAGGCATGTGGACTCAGGAGGATGA</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq3                               </span></td></tr>\n",
       "\t<tr><td>GGCGCTGCCGGACTGCGCGTGGAGTGGCGCGCTGCT</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " V1\\\\\n",
       "\\hline\n",
       "\t >seq1                               \\\\\n",
       "\t GAAGCCCTGGCGGGGCGCGTGCCCGCCGCCGCCGCC\\\\\n",
       "\t >seq2                               \\\\\n",
       "\t TCCCGGGGCTAGAGGCATGTGGACTCAGGAGGATGA\\\\\n",
       "\t >seq3                               \\\\\n",
       "\t GGCGCTGCCGGACTGCGCGTGGAGTGGCGCGCTGCT\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| V1 |\n",
       "|---|\n",
       "| >seq1                                |\n",
       "| GAAGCCCTGGCGGGGCGCGTGCCCGCCGCCGCCGCC |\n",
       "| >seq2                                |\n",
       "| TCCCGGGGCTAGAGGCATGTGGACTCAGGAGGATGA |\n",
       "| >seq3                                |\n",
       "| GGCGCTGCCGGACTGCGCGTGGAGTGGCGCGCTGCT |\n",
       "\n"
      ],
      "text/plain": [
       "  V1                                  \n",
       "1 >seq1                               \n",
       "2 GAAGCCCTGGCGGGGCGCGTGCCCGCCGCCGCCGCC\n",
       "3 >seq2                               \n",
       "4 TCCCGGGGCTAGAGGCATGTGGACTCAGGAGGATGA\n",
       "5 >seq3                               \n",
       "6 GGCGCTGCCGGACTGCGCGTGGAGTGGCGCGCTGCT"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>17136</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 17136\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 17136\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 17136     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Myc"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq1                               </span></td></tr>\n",
       "\t<tr><td>ACCGACCGGCGCGGGCACGAGGCAATGGCGGCCGGG</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq2                               </span></td></tr>\n",
       "\t<tr><td>AACAGCGCCACCGGCCTCGTGCACTTCTTCCACTGT</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>&gt;seq3                               </span></td></tr>\n",
       "\t<tr><td>GCGGCCGGTCTGCACCATGCTGCGAACGTCCGTCCT</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " V1\\\\\n",
       "\\hline\n",
       "\t >seq1                               \\\\\n",
       "\t ACCGACCGGCGCGGGCACGAGGCAATGGCGGCCGGG\\\\\n",
       "\t >seq2                               \\\\\n",
       "\t AACAGCGCCACCGGCCTCGTGCACTTCTTCCACTGT\\\\\n",
       "\t >seq3                               \\\\\n",
       "\t GCGGCCGGTCTGCACCATGCTGCGAACGTCCGTCCT\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| V1 |\n",
       "|---|\n",
       "| >seq1                                |\n",
       "| ACCGACCGGCGCGGGCACGAGGCAATGGCGGCCGGG |\n",
       "| >seq2                                |\n",
       "| AACAGCGCCACCGGCCTCGTGCACTTCTTCCACTGT |\n",
       "| >seq3                                |\n",
       "| GCGGCCGGTCTGCACCATGCTGCGAACGTCCGTCCT |\n",
       "\n"
      ],
      "text/plain": [
       "  V1                                  \n",
       "1 >seq1                               \n",
       "2 ACCGACCGGCGCGGGCACGAGGCAATGGCGGCCGGG\n",
       "3 >seq2                               \n",
       "4 AACAGCGCCACCGGCCTCGTGCACTTCTTCCACTGT\n",
       "5 >seq3                               \n",
       "6 GCGGCCGGTCTGCACCATGCTGCGAACGTCCGTCCT"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>13852</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 13852\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 13852\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 13852     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat('Mad')\n",
    "head(read.table('Mad.fa'))\n",
    "dim(read.table('Mad.fa'))\n",
    "cat('Max')\n",
    "head(read.table('Max.fa'))\n",
    "dim(read.table('Max.fa'))\n",
    "cat('Myc')\n",
    "head(read.table('Myc.fa'))\n",
    "dim(read.table('Myc.fa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the shape using DNAShapeR\n",
    "Using the DNAShapeR package, the shape of the sequence was predicted. The shape includes intra-base pair parameters like minor groove width (MGW) and propeller twist (ProT) as well as inter-base pair parameters like roll (Roll) and helical twist (HelT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files......\n",
      "Record length: 36\n",
      "Record length: 35\n",
      "Record length: 36\n",
      "Record length: 35\n",
      "Record length: 36\n",
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files......\n",
      "Record length: 36\n",
      "Record length: 35\n",
      "Record length: 36\n",
      "Record length: 35\n",
      "Record length: 36\n",
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n",
      "Reading the input sequence......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files......\n",
      "Record length: 36\n",
      "Record length: 35\n",
      "Record length: 36\n",
      "Record length: 35\n",
      "Record length: 36\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# list for each shape output\n",
    "shapes <- list()\n",
    "# get shape for each file\n",
    "for (n in ns) {\n",
    "    fn <- paste(n,'.fa', sep='')\n",
    "    shapes[[length(shapes) + 1]] <- getShape(fn)\n",
    "}\n",
    "# add names\n",
    "names(shapes) <- ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview of the shape data\n",
    "For each of the TFs, matrix for the shape is created. For intra-base pair parameters, the dimensions are m x 36, whereas for inter-base pair parameters, the dimensions are m x 35. Here, m is the number of sequences (ie number of rows with sequences in the FASTA file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGW 7534 36"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>NA  </td><td>NA  </td><td>5.02</td><td>5.33</td><td>5.50</td><td>5.27</td><td>5.50</td><td>5.40</td><td>4.36</td><td>4.03</td><td>... </td><td>5.78</td><td>5.79</td><td>5.67</td><td>5.27</td><td>5.08</td><td>5.45</td><td>4.52</td><td>4.14</td><td>NA  </td><td>NA  </td></tr>\n",
       "\t<tr><td>NA  </td><td>NA  </td><td>4.94</td><td>4.76</td><td>4.85</td><td>4.93</td><td>4.80</td><td>4.63</td><td>5.05</td><td>4.67</td><td>... </td><td>5.27</td><td>5.08</td><td>5.45</td><td>4.52</td><td>3.74</td><td>4.51</td><td>4.81</td><td>4.92</td><td>NA  </td><td>NA  </td></tr>\n",
       "\t<tr><td>NA  </td><td>NA  </td><td>5.29</td><td>5.37</td><td>4.96</td><td>4.77</td><td>4.95</td><td>5.19</td><td>5.19</td><td>4.95</td><td>... </td><td>4.95</td><td>4.61</td><td>4.67</td><td>5.05</td><td>4.63</td><td>4.92</td><td>5.40</td><td>5.19</td><td>NA  </td><td>NA  </td></tr>\n",
       "\t<tr><td>NA  </td><td>NA  </td><td>5.59</td><td>5.37</td><td>4.83</td><td>4.17</td><td>4.82</td><td>4.92</td><td>5.38</td><td>5.43</td><td>... </td><td>4.98</td><td>5.69</td><td>5.69</td><td>4.68</td><td>4.33</td><td>5.43</td><td>5.62</td><td>5.01</td><td>NA  </td><td>NA  </td></tr>\n",
       "\t<tr><td>NA  </td><td>NA  </td><td>5.10</td><td>5.20</td><td>5.46</td><td>5.40</td><td>4.77</td><td>4.94</td><td>5.52</td><td>5.41</td><td>... </td><td>4.96</td><td>4.77</td><td>4.82</td><td>4.62</td><td>4.42</td><td>4.05</td><td>4.52</td><td>5.21</td><td>NA  </td><td>NA  </td></tr>\n",
       "\t<tr><td>NA  </td><td>NA  </td><td>5.67</td><td>5.13</td><td>5.15</td><td>5.37</td><td>4.96</td><td>4.77</td><td>4.82</td><td>4.89</td><td>... </td><td>4.14</td><td>5.31</td><td>5.41</td><td>4.66</td><td>4.77</td><td>5.56</td><td>5.46</td><td>4.93</td><td>NA  </td><td>NA  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllllllllll}\n",
       "\t NA   & NA   & 5.02 & 5.33 & 5.50 & 5.27 & 5.50 & 5.40 & 4.36 & 4.03 & ...  & 5.78 & 5.79 & 5.67 & 5.27 & 5.08 & 5.45 & 4.52 & 4.14 & NA   & NA  \\\\\n",
       "\t NA   & NA   & 4.94 & 4.76 & 4.85 & 4.93 & 4.80 & 4.63 & 5.05 & 4.67 & ...  & 5.27 & 5.08 & 5.45 & 4.52 & 3.74 & 4.51 & 4.81 & 4.92 & NA   & NA  \\\\\n",
       "\t NA   & NA   & 5.29 & 5.37 & 4.96 & 4.77 & 4.95 & 5.19 & 5.19 & 4.95 & ...  & 4.95 & 4.61 & 4.67 & 5.05 & 4.63 & 4.92 & 5.40 & 5.19 & NA   & NA  \\\\\n",
       "\t NA   & NA   & 5.59 & 5.37 & 4.83 & 4.17 & 4.82 & 4.92 & 5.38 & 5.43 & ...  & 4.98 & 5.69 & 5.69 & 4.68 & 4.33 & 5.43 & 5.62 & 5.01 & NA   & NA  \\\\\n",
       "\t NA   & NA   & 5.10 & 5.20 & 5.46 & 5.40 & 4.77 & 4.94 & 5.52 & 5.41 & ...  & 4.96 & 4.77 & 4.82 & 4.62 & 4.42 & 4.05 & 4.52 & 5.21 & NA   & NA  \\\\\n",
       "\t NA   & NA   & 5.67 & 5.13 & 5.15 & 5.37 & 4.96 & 4.77 & 4.82 & 4.89 & ...  & 4.14 & 5.31 & 5.41 & 4.66 & 4.77 & 5.56 & 5.46 & 4.93 & NA   & NA  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| NA   | NA   | 5.02 | 5.33 | 5.50 | 5.27 | 5.50 | 5.40 | 4.36 | 4.03 | ...  | 5.78 | 5.79 | 5.67 | 5.27 | 5.08 | 5.45 | 4.52 | 4.14 | NA   | NA   |\n",
       "| NA   | NA   | 4.94 | 4.76 | 4.85 | 4.93 | 4.80 | 4.63 | 5.05 | 4.67 | ...  | 5.27 | 5.08 | 5.45 | 4.52 | 3.74 | 4.51 | 4.81 | 4.92 | NA   | NA   |\n",
       "| NA   | NA   | 5.29 | 5.37 | 4.96 | 4.77 | 4.95 | 5.19 | 5.19 | 4.95 | ...  | 4.95 | 4.61 | 4.67 | 5.05 | 4.63 | 4.92 | 5.40 | 5.19 | NA   | NA   |\n",
       "| NA   | NA   | 5.59 | 5.37 | 4.83 | 4.17 | 4.82 | 4.92 | 5.38 | 5.43 | ...  | 4.98 | 5.69 | 5.69 | 4.68 | 4.33 | 5.43 | 5.62 | 5.01 | NA   | NA   |\n",
       "| NA   | NA   | 5.10 | 5.20 | 5.46 | 5.40 | 4.77 | 4.94 | 5.52 | 5.41 | ...  | 4.96 | 4.77 | 4.82 | 4.62 | 4.42 | 4.05 | 4.52 | 5.21 | NA   | NA   |\n",
       "| NA   | NA   | 5.67 | 5.13 | 5.15 | 5.37 | 4.96 | 4.77 | 4.82 | 4.89 | ...  | 4.14 | 5.31 | 5.41 | 4.66 | 4.77 | 5.56 | 5.46 | 4.93 | NA   | NA   |\n",
       "\n"
      ],
      "text/plain": [
       "  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
       "1 NA   NA   5.02 5.33 5.50 5.27 5.50 5.40 4.36 4.03  ...   5.78  5.79  5.67 \n",
       "2 NA   NA   4.94 4.76 4.85 4.93 4.80 4.63 5.05 4.67  ...   5.27  5.08  5.45 \n",
       "3 NA   NA   5.29 5.37 4.96 4.77 4.95 5.19 5.19 4.95  ...   4.95  4.61  4.67 \n",
       "4 NA   NA   5.59 5.37 4.83 4.17 4.82 4.92 5.38 5.43  ...   4.98  5.69  5.69 \n",
       "5 NA   NA   5.10 5.20 5.46 5.40 4.77 4.94 5.52 5.41  ...   4.96  4.77  4.82 \n",
       "6 NA   NA   5.67 5.13 5.15 5.37 4.96 4.77 4.82 4.89  ...   4.14  5.31  5.41 \n",
       "  [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n",
       "1 5.27  5.08  5.45  4.52  4.14  NA    NA   \n",
       "2 4.52  3.74  4.51  4.81  4.92  NA    NA   \n",
       "3 5.05  4.63  4.92  5.40  5.19  NA    NA   \n",
       "4 4.68  4.33  5.43  5.62  5.01  NA    NA   \n",
       "5 4.62  4.42  4.05  4.52  5.21  NA    NA   \n",
       "6 4.66  4.77  5.56  5.46  4.93  NA    NA   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProT 7534 36"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>NA    </td><td>NA    </td><td>-2.52 </td><td>-3.12 </td><td>-9.43 </td><td>-9.26 </td><td>-6.07 </td><td>-10.81</td><td>-11.93</td><td>-10.58</td><td>...   </td><td>-3.80 </td><td>-3.68 </td><td>-6.76 </td><td> -1.72</td><td>-3.36 </td><td>-9.90 </td><td>-10.21</td><td> -4.48</td><td>NA    </td><td>NA    </td></tr>\n",
       "\t<tr><td>NA    </td><td>NA    </td><td>-1.95 </td><td>-1.44 </td><td>-4.60 </td><td>-5.75 </td><td>-0.44 </td><td> -4.81</td><td> -6.07</td><td> -0.53</td><td>...   </td><td>-1.72 </td><td>-3.36 </td><td>-9.90 </td><td>-10.21</td><td>-4.07 </td><td>-7.34 </td><td> -5.39</td><td> -0.90</td><td>NA    </td><td>NA    </td></tr>\n",
       "\t<tr><td>NA    </td><td>NA    </td><td>-3.13 </td><td>-3.73 </td><td>-2.72 </td><td>-1.93 </td><td>-2.73 </td><td> -3.26</td><td> -3.26</td><td> -2.73</td><td>...   </td><td>-2.73 </td><td>-2.48 </td><td>-0.53 </td><td> -6.07</td><td>-4.81 </td><td>-0.90 </td><td> -5.54</td><td> -4.44</td><td>NA    </td><td>NA    </td></tr>\n",
       "\t<tr><td>NA    </td><td>NA    </td><td>-4.13 </td><td>-3.73 </td><td>-3.04 </td><td>-1.55 </td><td>-8.01 </td><td> -9.80</td><td> -6.08</td><td> -3.40</td><td>...   </td><td>-1.39 </td><td>-6.51 </td><td>-6.51 </td><td> -1.54</td><td>-5.43 </td><td>-7.21 </td><td>-10.31</td><td> -8.92</td><td>NA    </td><td>NA    </td></tr>\n",
       "\t<tr><td>NA    </td><td>NA    </td><td>-0.94 </td><td>-6.78 </td><td>-6.31 </td><td>-2.99 </td><td>-2.82 </td><td> -1.28</td><td> -5.92</td><td> -3.20</td><td>...   </td><td>-2.72 </td><td>-1.93 </td><td>-2.13 </td><td> -1.32</td><td>-0.14 </td><td>-9.21 </td><td>-12.85</td><td>-12.03</td><td>NA    </td><td>NA    </td></tr>\n",
       "\t<tr><td>NA    </td><td>NA    </td><td>-6.76 </td><td>-1.72 </td><td>-3.78 </td><td>-3.73 </td><td>-2.72 </td><td> -1.93</td><td> -2.13</td><td> -2.02</td><td>...   </td><td>-4.48 </td><td>-7.40 </td><td>-2.94 </td><td> -3.38</td><td>-5.89 </td><td>-7.11 </td><td> -4.03</td><td> -2.54</td><td>NA    </td><td>NA    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllllllllll}\n",
       "\t NA     & NA     & -2.52  & -3.12  & -9.43  & -9.26  & -6.07  & -10.81 & -11.93 & -10.58 & ...    & -3.80  & -3.68  & -6.76  &  -1.72 & -3.36  & -9.90  & -10.21 &  -4.48 & NA     & NA    \\\\\n",
       "\t NA     & NA     & -1.95  & -1.44  & -4.60  & -5.75  & -0.44  &  -4.81 &  -6.07 &  -0.53 & ...    & -1.72  & -3.36  & -9.90  & -10.21 & -4.07  & -7.34  &  -5.39 &  -0.90 & NA     & NA    \\\\\n",
       "\t NA     & NA     & -3.13  & -3.73  & -2.72  & -1.93  & -2.73  &  -3.26 &  -3.26 &  -2.73 & ...    & -2.73  & -2.48  & -0.53  &  -6.07 & -4.81  & -0.90  &  -5.54 &  -4.44 & NA     & NA    \\\\\n",
       "\t NA     & NA     & -4.13  & -3.73  & -3.04  & -1.55  & -8.01  &  -9.80 &  -6.08 &  -3.40 & ...    & -1.39  & -6.51  & -6.51  &  -1.54 & -5.43  & -7.21  & -10.31 &  -8.92 & NA     & NA    \\\\\n",
       "\t NA     & NA     & -0.94  & -6.78  & -6.31  & -2.99  & -2.82  &  -1.28 &  -5.92 &  -3.20 & ...    & -2.72  & -1.93  & -2.13  &  -1.32 & -0.14  & -9.21  & -12.85 & -12.03 & NA     & NA    \\\\\n",
       "\t NA     & NA     & -6.76  & -1.72  & -3.78  & -3.73  & -2.72  &  -1.93 &  -2.13 &  -2.02 & ...    & -4.48  & -7.40  & -2.94  &  -3.38 & -5.89  & -7.11  &  -4.03 &  -2.54 & NA     & NA    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| NA     | NA     | -2.52  | -3.12  | -9.43  | -9.26  | -6.07  | -10.81 | -11.93 | -10.58 | ...    | -3.80  | -3.68  | -6.76  |  -1.72 | -3.36  | -9.90  | -10.21 |  -4.48 | NA     | NA     |\n",
       "| NA     | NA     | -1.95  | -1.44  | -4.60  | -5.75  | -0.44  |  -4.81 |  -6.07 |  -0.53 | ...    | -1.72  | -3.36  | -9.90  | -10.21 | -4.07  | -7.34  |  -5.39 |  -0.90 | NA     | NA     |\n",
       "| NA     | NA     | -3.13  | -3.73  | -2.72  | -1.93  | -2.73  |  -3.26 |  -3.26 |  -2.73 | ...    | -2.73  | -2.48  | -0.53  |  -6.07 | -4.81  | -0.90  |  -5.54 |  -4.44 | NA     | NA     |\n",
       "| NA     | NA     | -4.13  | -3.73  | -3.04  | -1.55  | -8.01  |  -9.80 |  -6.08 |  -3.40 | ...    | -1.39  | -6.51  | -6.51  |  -1.54 | -5.43  | -7.21  | -10.31 |  -8.92 | NA     | NA     |\n",
       "| NA     | NA     | -0.94  | -6.78  | -6.31  | -2.99  | -2.82  |  -1.28 |  -5.92 |  -3.20 | ...    | -2.72  | -1.93  | -2.13  |  -1.32 | -0.14  | -9.21  | -12.85 | -12.03 | NA     | NA     |\n",
       "| NA     | NA     | -6.76  | -1.72  | -3.78  | -3.73  | -2.72  |  -1.93 |  -2.13 |  -2.02 | ...    | -4.48  | -7.40  | -2.94  |  -3.38 | -5.89  | -7.11  |  -4.03 |  -2.54 | NA     | NA     |\n",
       "\n"
      ],
      "text/plain": [
       "  [,1] [,2] [,3]  [,4]  [,5]  [,6]  [,7]  [,8]   [,9]   [,10]  [,11] [,12]\n",
       "1 NA   NA   -2.52 -3.12 -9.43 -9.26 -6.07 -10.81 -11.93 -10.58 ...   -3.80\n",
       "2 NA   NA   -1.95 -1.44 -4.60 -5.75 -0.44  -4.81  -6.07  -0.53 ...   -1.72\n",
       "3 NA   NA   -3.13 -3.73 -2.72 -1.93 -2.73  -3.26  -3.26  -2.73 ...   -2.73\n",
       "4 NA   NA   -4.13 -3.73 -3.04 -1.55 -8.01  -9.80  -6.08  -3.40 ...   -1.39\n",
       "5 NA   NA   -0.94 -6.78 -6.31 -2.99 -2.82  -1.28  -5.92  -3.20 ...   -2.72\n",
       "6 NA   NA   -6.76 -1.72 -3.78 -3.73 -2.72  -1.93  -2.13  -2.02 ...   -4.48\n",
       "  [,13] [,14] [,15]  [,16] [,17] [,18]  [,19]  [,20] [,21]\n",
       "1 -3.68 -6.76  -1.72 -3.36 -9.90 -10.21  -4.48 NA    NA   \n",
       "2 -3.36 -9.90 -10.21 -4.07 -7.34  -5.39  -0.90 NA    NA   \n",
       "3 -2.48 -0.53  -6.07 -4.81 -0.90  -5.54  -4.44 NA    NA   \n",
       "4 -6.51 -6.51  -1.54 -5.43 -7.21 -10.31  -8.92 NA    NA   \n",
       "5 -1.93 -2.13  -1.32 -0.14 -9.21 -12.85 -12.03 NA    NA   \n",
       "6 -7.40 -2.94  -3.38 -5.89 -7.11  -4.03  -2.54 NA    NA   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roll 7534 35"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>NA   </td><td>-1.78</td><td>-1.87</td><td> 4.47</td><td>-4.16</td><td> 5.82</td><td> 0.27</td><td>-2.32</td><td>-5.01</td><td>-3.83</td><td>...  </td><td> 4.28</td><td>-0.97</td><td> 3.49</td><td>-1.98</td><td>-2.27</td><td> 4.12</td><td>-3.49</td><td>-2.57</td><td>-4.51</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>-1.77</td><td>-2.04</td><td>-1.11</td><td>-1.89</td><td>-3.06</td><td>-1.35</td><td>-2.10</td><td>-2.75</td><td>-3.46</td><td>...  </td><td>-1.98</td><td>-2.27</td><td> 4.12</td><td>-3.49</td><td>-3.18</td><td>-4.65</td><td>-2.33</td><td>-1.00</td><td>-2.79</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>-2.02</td><td> 2.29</td><td>-1.31</td><td>-2.20</td><td>-2.04</td><td>-1.65</td><td> 1.93</td><td>-1.65</td><td>-1.75</td><td>...  </td><td>-1.73</td><td>-1.84</td><td>-3.01</td><td>-2.75</td><td>-2.10</td><td>-1.19</td><td>-2.83</td><td> 2.76</td><td>-0.64</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>-0.88</td><td> 2.76</td><td>-1.38</td><td>-3.24</td><td>-3.25</td><td>-3.53</td><td>-0.35</td><td> 3.33</td><td>-1.21</td><td>...  </td><td>-1.19</td><td>-2.37</td><td> 4.92</td><td>-2.59</td><td>-2.29</td><td>-4.10</td><td> 5.46</td><td>-3.06</td><td>-2.56</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>-2.29</td><td>-2.33</td><td>-1.19</td><td> 3.83</td><td>-1.91</td><td>-1.85</td><td>-2.71</td><td> 3.17</td><td>-1.88</td><td>...  </td><td>-1.31</td><td>-2.20</td><td>-2.16</td><td>-2.21</td><td>-3.04</td><td>-4.21</td><td>-4.82</td><td>-2.68</td><td> 6.09</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td> 3.55</td><td>-2.01</td><td>-1.79</td><td> 2.61</td><td>-1.31</td><td>-2.20</td><td>-2.16</td><td>-2.06</td><td>-2.12</td><td>...  </td><td>-2.83</td><td>-3.73</td><td> 2.59</td><td>-1.93</td><td>-1.25</td><td>-2.77</td><td> 2.77</td><td>-1.64</td><td>-2.42</td><td>NA   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllll}\n",
       "\t NA    & -1.78 & -1.87 &  4.47 & -4.16 &  5.82 &  0.27 & -2.32 & -5.01 & -3.83 & ...   &  4.28 & -0.97 &  3.49 & -1.98 & -2.27 &  4.12 & -3.49 & -2.57 & -4.51 & NA   \\\\\n",
       "\t NA    & -1.77 & -2.04 & -1.11 & -1.89 & -3.06 & -1.35 & -2.10 & -2.75 & -3.46 & ...   & -1.98 & -2.27 &  4.12 & -3.49 & -3.18 & -4.65 & -2.33 & -1.00 & -2.79 & NA   \\\\\n",
       "\t NA    & -2.02 &  2.29 & -1.31 & -2.20 & -2.04 & -1.65 &  1.93 & -1.65 & -1.75 & ...   & -1.73 & -1.84 & -3.01 & -2.75 & -2.10 & -1.19 & -2.83 &  2.76 & -0.64 & NA   \\\\\n",
       "\t NA    & -0.88 &  2.76 & -1.38 & -3.24 & -3.25 & -3.53 & -0.35 &  3.33 & -1.21 & ...   & -1.19 & -2.37 &  4.92 & -2.59 & -2.29 & -4.10 &  5.46 & -3.06 & -2.56 & NA   \\\\\n",
       "\t NA    & -2.29 & -2.33 & -1.19 &  3.83 & -1.91 & -1.85 & -2.71 &  3.17 & -1.88 & ...   & -1.31 & -2.20 & -2.16 & -2.21 & -3.04 & -4.21 & -4.82 & -2.68 &  6.09 & NA   \\\\\n",
       "\t NA    &  3.55 & -2.01 & -1.79 &  2.61 & -1.31 & -2.20 & -2.16 & -2.06 & -2.12 & ...   & -2.83 & -3.73 &  2.59 & -1.93 & -1.25 & -2.77 &  2.77 & -1.64 & -2.42 & NA   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| NA    | -1.78 | -1.87 |  4.47 | -4.16 |  5.82 |  0.27 | -2.32 | -5.01 | -3.83 | ...   |  4.28 | -0.97 |  3.49 | -1.98 | -2.27 |  4.12 | -3.49 | -2.57 | -4.51 | NA    |\n",
       "| NA    | -1.77 | -2.04 | -1.11 | -1.89 | -3.06 | -1.35 | -2.10 | -2.75 | -3.46 | ...   | -1.98 | -2.27 |  4.12 | -3.49 | -3.18 | -4.65 | -2.33 | -1.00 | -2.79 | NA    |\n",
       "| NA    | -2.02 |  2.29 | -1.31 | -2.20 | -2.04 | -1.65 |  1.93 | -1.65 | -1.75 | ...   | -1.73 | -1.84 | -3.01 | -2.75 | -2.10 | -1.19 | -2.83 |  2.76 | -0.64 | NA    |\n",
       "| NA    | -0.88 |  2.76 | -1.38 | -3.24 | -3.25 | -3.53 | -0.35 |  3.33 | -1.21 | ...   | -1.19 | -2.37 |  4.92 | -2.59 | -2.29 | -4.10 |  5.46 | -3.06 | -2.56 | NA    |\n",
       "| NA    | -2.29 | -2.33 | -1.19 |  3.83 | -1.91 | -1.85 | -2.71 |  3.17 | -1.88 | ...   | -1.31 | -2.20 | -2.16 | -2.21 | -3.04 | -4.21 | -4.82 | -2.68 |  6.09 | NA    |\n",
       "| NA    |  3.55 | -2.01 | -1.79 |  2.61 | -1.31 | -2.20 | -2.16 | -2.06 | -2.12 | ...   | -2.83 | -3.73 |  2.59 | -1.93 | -1.25 | -2.77 |  2.77 | -1.64 | -2.42 | NA    |\n",
       "\n"
      ],
      "text/plain": [
       "  [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11] [,12] [,13]\n",
       "1 NA   -1.78 -1.87  4.47 -4.16  5.82  0.27 -2.32 -5.01 -3.83 ...    4.28 -0.97\n",
       "2 NA   -1.77 -2.04 -1.11 -1.89 -3.06 -1.35 -2.10 -2.75 -3.46 ...   -1.98 -2.27\n",
       "3 NA   -2.02  2.29 -1.31 -2.20 -2.04 -1.65  1.93 -1.65 -1.75 ...   -1.73 -1.84\n",
       "4 NA   -0.88  2.76 -1.38 -3.24 -3.25 -3.53 -0.35  3.33 -1.21 ...   -1.19 -2.37\n",
       "5 NA   -2.29 -2.33 -1.19  3.83 -1.91 -1.85 -2.71  3.17 -1.88 ...   -1.31 -2.20\n",
       "6 NA    3.55 -2.01 -1.79  2.61 -1.31 -2.20 -2.16 -2.06 -2.12 ...   -2.83 -3.73\n",
       "  [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n",
       "1  3.49 -1.98 -2.27  4.12 -3.49 -2.57 -4.51 NA   \n",
       "2  4.12 -3.49 -3.18 -4.65 -2.33 -1.00 -2.79 NA   \n",
       "3 -3.01 -2.75 -2.10 -1.19 -2.83  2.76 -0.64 NA   \n",
       "4  4.92 -2.59 -2.29 -4.10  5.46 -3.06 -2.56 NA   \n",
       "5 -2.16 -2.21 -3.04 -4.21 -4.82 -2.68  6.09 NA   \n",
       "6  2.59 -1.93 -1.25 -2.77  2.77 -1.64 -2.42 NA   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelT 7534 35"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>NA   </td><td>33.82</td><td>36.28</td><td>34.74</td><td>31.64</td><td>35.02</td><td>35.14</td><td>35.91</td><td>36.33</td><td>32.65</td><td>...  </td><td>34.48</td><td>35.56</td><td>34.50</td><td>31.26</td><td>36.80</td><td>34.39</td><td>34.91</td><td>33.12</td><td>34.91</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>33.22</td><td>33.58</td><td>33.57</td><td>35.58</td><td>31.48</td><td>34.15</td><td>35.99</td><td>31.38</td><td>37.36</td><td>...  </td><td>31.29</td><td>36.80</td><td>34.39</td><td>34.91</td><td>33.14</td><td>35.20</td><td>35.91</td><td>34.36</td><td>31.21</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>36.31</td><td>32.39</td><td>33.33</td><td>36.73</td><td>33.67</td><td>33.34</td><td>32.53</td><td>33.34</td><td>33.80</td><td>...  </td><td>33.51</td><td>33.64</td><td>37.20</td><td>31.38</td><td>35.99</td><td>34.17</td><td>31.19</td><td>34.13</td><td>34.39</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>35.63</td><td>32.80</td><td>33.36</td><td>37.48</td><td>32.28</td><td>35.03</td><td>35.42</td><td>32.57</td><td>33.11</td><td>...  </td><td>34.50</td><td>31.56</td><td>34.19</td><td>31.46</td><td>35.12</td><td>34.62</td><td>34.49</td><td>34.86</td><td>32.17</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>36.61</td><td>31.44</td><td>35.22</td><td>32.59</td><td>36.26</td><td>34.17</td><td>31.16</td><td>33.84</td><td>36.24</td><td>...  </td><td>33.33</td><td>36.73</td><td>33.62</td><td>33.59</td><td>34.00</td><td>32.37</td><td>36.09</td><td>35.59</td><td>35.41</td><td>NA   </td></tr>\n",
       "\t<tr><td>NA   </td><td>34.49</td><td>31.24</td><td>36.70</td><td>32.40</td><td>33.33</td><td>36.73</td><td>33.62</td><td>33.53</td><td>33.36</td><td>...  </td><td>33.08</td><td>34.64</td><td>34.10</td><td>33.54</td><td>34.52</td><td>34.03</td><td>34.23</td><td>33.17</td><td>37.04</td><td>NA   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllll}\n",
       "\t NA    & 33.82 & 36.28 & 34.74 & 31.64 & 35.02 & 35.14 & 35.91 & 36.33 & 32.65 & ...   & 34.48 & 35.56 & 34.50 & 31.26 & 36.80 & 34.39 & 34.91 & 33.12 & 34.91 & NA   \\\\\n",
       "\t NA    & 33.22 & 33.58 & 33.57 & 35.58 & 31.48 & 34.15 & 35.99 & 31.38 & 37.36 & ...   & 31.29 & 36.80 & 34.39 & 34.91 & 33.14 & 35.20 & 35.91 & 34.36 & 31.21 & NA   \\\\\n",
       "\t NA    & 36.31 & 32.39 & 33.33 & 36.73 & 33.67 & 33.34 & 32.53 & 33.34 & 33.80 & ...   & 33.51 & 33.64 & 37.20 & 31.38 & 35.99 & 34.17 & 31.19 & 34.13 & 34.39 & NA   \\\\\n",
       "\t NA    & 35.63 & 32.80 & 33.36 & 37.48 & 32.28 & 35.03 & 35.42 & 32.57 & 33.11 & ...   & 34.50 & 31.56 & 34.19 & 31.46 & 35.12 & 34.62 & 34.49 & 34.86 & 32.17 & NA   \\\\\n",
       "\t NA    & 36.61 & 31.44 & 35.22 & 32.59 & 36.26 & 34.17 & 31.16 & 33.84 & 36.24 & ...   & 33.33 & 36.73 & 33.62 & 33.59 & 34.00 & 32.37 & 36.09 & 35.59 & 35.41 & NA   \\\\\n",
       "\t NA    & 34.49 & 31.24 & 36.70 & 32.40 & 33.33 & 36.73 & 33.62 & 33.53 & 33.36 & ...   & 33.08 & 34.64 & 34.10 & 33.54 & 34.52 & 34.03 & 34.23 & 33.17 & 37.04 & NA   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| NA    | 33.82 | 36.28 | 34.74 | 31.64 | 35.02 | 35.14 | 35.91 | 36.33 | 32.65 | ...   | 34.48 | 35.56 | 34.50 | 31.26 | 36.80 | 34.39 | 34.91 | 33.12 | 34.91 | NA    |\n",
       "| NA    | 33.22 | 33.58 | 33.57 | 35.58 | 31.48 | 34.15 | 35.99 | 31.38 | 37.36 | ...   | 31.29 | 36.80 | 34.39 | 34.91 | 33.14 | 35.20 | 35.91 | 34.36 | 31.21 | NA    |\n",
       "| NA    | 36.31 | 32.39 | 33.33 | 36.73 | 33.67 | 33.34 | 32.53 | 33.34 | 33.80 | ...   | 33.51 | 33.64 | 37.20 | 31.38 | 35.99 | 34.17 | 31.19 | 34.13 | 34.39 | NA    |\n",
       "| NA    | 35.63 | 32.80 | 33.36 | 37.48 | 32.28 | 35.03 | 35.42 | 32.57 | 33.11 | ...   | 34.50 | 31.56 | 34.19 | 31.46 | 35.12 | 34.62 | 34.49 | 34.86 | 32.17 | NA    |\n",
       "| NA    | 36.61 | 31.44 | 35.22 | 32.59 | 36.26 | 34.17 | 31.16 | 33.84 | 36.24 | ...   | 33.33 | 36.73 | 33.62 | 33.59 | 34.00 | 32.37 | 36.09 | 35.59 | 35.41 | NA    |\n",
       "| NA    | 34.49 | 31.24 | 36.70 | 32.40 | 33.33 | 36.73 | 33.62 | 33.53 | 33.36 | ...   | 33.08 | 34.64 | 34.10 | 33.54 | 34.52 | 34.03 | 34.23 | 33.17 | 37.04 | NA    |\n",
       "\n"
      ],
      "text/plain": [
       "  [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11] [,12] [,13]\n",
       "1 NA   33.82 36.28 34.74 31.64 35.02 35.14 35.91 36.33 32.65 ...   34.48 35.56\n",
       "2 NA   33.22 33.58 33.57 35.58 31.48 34.15 35.99 31.38 37.36 ...   31.29 36.80\n",
       "3 NA   36.31 32.39 33.33 36.73 33.67 33.34 32.53 33.34 33.80 ...   33.51 33.64\n",
       "4 NA   35.63 32.80 33.36 37.48 32.28 35.03 35.42 32.57 33.11 ...   34.50 31.56\n",
       "5 NA   36.61 31.44 35.22 32.59 36.26 34.17 31.16 33.84 36.24 ...   33.33 36.73\n",
       "6 NA   34.49 31.24 36.70 32.40 33.33 36.73 33.62 33.53 33.36 ...   33.08 34.64\n",
       "  [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n",
       "1 34.50 31.26 36.80 34.39 34.91 33.12 34.91 NA   \n",
       "2 34.39 34.91 33.14 35.20 35.91 34.36 31.21 NA   \n",
       "3 37.20 31.38 35.99 34.17 31.19 34.13 34.39 NA   \n",
       "4 34.19 31.46 35.12 34.62 34.49 34.86 32.17 NA   \n",
       "5 33.62 33.59 34.00 32.37 36.09 35.59 35.41 NA   \n",
       "6 34.10 33.54 34.52 34.03 34.23 33.17 37.04 NA   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat('MGW', dim(shapes[[1]]$MGW))\n",
    "head(shapes[[1]]$MGW)\n",
    "cat('ProT', dim(shapes[[1]]$ProT))\n",
    "head(shapes[[1]]$ProT)\n",
    "cat('Roll', dim(shapes[[1]]$Roll))\n",
    "head(shapes[[1]]$Roll)\n",
    "cat('HelT', dim(shapes[[1]]$HelT))\n",
    "head(shapes[[1]]$HelT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the models, dataset containing training examples were created. The 2 separate datsets were created, one with only the sequence as input and the other with sequence + shape features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding sequence\n",
    "To create the dataset, the sequences need to be converted into numeric values. This was achieved by using one-hot encoding where 4-bit binary vector represents each letter as follows:\n",
    "\n",
    "A: [0,0,0,1] <br>\n",
    "C: [0,0,1,0] <br>\n",
    "G: [0,1,0,0] <br>\n",
    "T: [1,0,0,0] <br>\n",
    "\n",
    "Using this, the sequence ACCTG would be [0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0]\n",
    "\n",
    "Below is the function that was used to encode the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode nucleotide\n",
    "# gets nucleotide and returns the corresponding one-hot encoding\n",
    "encodeNucleotide <- function(nucleotide) {\n",
    "    dict <- list(A=c(0,0,0,1), C=c(0,0,1,0), G=c(0,1,0,0), T=c(1,0,0,0))\n",
    "    return(dict[nucleotide])\n",
    "}\n",
    "\n",
    "# encode sequence\n",
    "# gets sequence and returns vector of one-hot encoded sequence\n",
    "encodeSequence <- function(sequence) {\n",
    "    nucleotides <- strsplit(sequence, split='', fixed=T)\n",
    "    result <- unlist(lapply(nucleotides, encodeNucleotide))\n",
    "    names(result) <- c()\n",
    "    return(result)\n",
    "}\n",
    "\n",
    "# encode FASTA\n",
    "# gets FASTA file and returns matrix of one-hot encoded sequences\n",
    "encodeFASTA <- function(filename) {\n",
    "    # read FASTA\n",
    "    fa <- read.table(filename, comment.char='>')\n",
    "    # encode sequence\n",
    "    result <- apply(fa, 1, encodeSequence)\n",
    "    # transpose\n",
    "    result <- t(result)\n",
    "    return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating feature matrix\n",
    "Feature matrix for each TF was created as inputs to the model. For the sequence only model, the features were the one-hot encoded sequences. For the sequence + shape model, the one-hot encoded sequences were combined with shape features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of feature matrices for each file\n",
    "features.seq <- list()\n",
    "features.shape <- list()\n",
    "\n",
    "# loop over files\n",
    "for (n in ns) {\n",
    "    # add 1-mer feature vector\n",
    "    features.seq[[n]] <- encodeFASTA(paste(n, '.fa', sep=''))\n",
    "    # combine intra bp parameters\n",
    "    features.shape[[n]] <- cbind(features.seq[[n]], shapes[[n]]$MGW, shapes[[n]]$ProT)\n",
    "    # combine inter bp parameters\n",
    "    features.shape[[n]] <- cbind(features.shape[[n]], shapes[[n]]$Roll, shapes[[n]]$HelT)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove NA values\n",
    "Since the model cannot compute NA values, columns with NA values were dropped (ie columns in inter-base parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with NA\n",
    "for (n in ns) {\n",
    "    features.seq[[n]] <- features.seq[[n]][, colSums(is.na(features.seq[[n]])) == 0]\n",
    "    features.shape[[n]] <- features.shape[[n]][, colSums(is.na(features.shape[[n]])) == 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add ground truth\n",
    "To learn the weights so that the model can minimize the error, ground truth was added to the dataset. Ground truth values were obtained from High-Throughput protein-DNA binding data in the form of `*.s` files. These files contain the sequences in the `*.fa` files and their corresponding binding affinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th><th scope=col>V2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG</td><td> 8.763428                           </td></tr>\n",
       "\t<tr><td>CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG</td><td> 7.903412                           </td></tr>\n",
       "\t<tr><td>GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT</td><td>10.208874                           </td></tr>\n",
       "\t<tr><td>TGCGGCTTCGGCTTCCACGCGGCATCCTAGGTAAGC</td><td> 8.273974                           </td></tr>\n",
       "\t<tr><td>TGCTCGCCTGCCTGCCATGTGCAGCGGCCCCTTTGT</td><td> 8.325185                           </td></tr>\n",
       "\t<tr><td>GCAGCGGCCCCATCCCACGTGGTTAAGTGGGTGGCC</td><td>10.389595                           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " V1 & V2\\\\\n",
       "\\hline\n",
       "\t GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG &  8.763428                           \\\\\n",
       "\t CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG &  7.903412                           \\\\\n",
       "\t GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT & 10.208874                           \\\\\n",
       "\t TGCGGCTTCGGCTTCCACGCGGCATCCTAGGTAAGC &  8.273974                           \\\\\n",
       "\t TGCTCGCCTGCCTGCCATGTGCAGCGGCCCCTTTGT &  8.325185                           \\\\\n",
       "\t GCAGCGGCCCCATCCCACGTGGTTAAGTGGGTGGCC & 10.389595                           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| V1 | V2 |\n",
       "|---|---|\n",
       "| GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG |  8.763428                            |\n",
       "| CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG |  7.903412                            |\n",
       "| GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT | 10.208874                            |\n",
       "| TGCGGCTTCGGCTTCCACGCGGCATCCTAGGTAAGC |  8.273974                            |\n",
       "| TGCTCGCCTGCCTGCCATGTGCAGCGGCCCCTTTGT |  8.325185                            |\n",
       "| GCAGCGGCCCCATCCCACGTGGTTAAGTGGGTGGCC | 10.389595                            |\n",
       "\n"
      ],
      "text/plain": [
       "  V1                                   V2       \n",
       "1 GGGCATGAAAGCCACCTCGTGGTTTTGCAGCAAGTG  8.763428\n",
       "2 CGGGGAGGAGCCACCCACGCGCCACAGCAAGTCCTG  7.903412\n",
       "3 GGCGGCCCGGGCATCCACGTGGGTCGGGCTCCTGGT 10.208874\n",
       "4 TGCGGCTTCGGCTTCCACGCGGCATCCTAGGTAAGC  8.273974\n",
       "5 TGCTCGCCTGCCTGCCATGTGCAGCGGCCCCTTTGT  8.325185\n",
       "6 GCAGCGGCCCCATCCCACGTGGTTAAGTGGGTGGCC 10.389595"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(read.table('Mad.s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ground truth to features\n",
    "data.seq <- list()\n",
    "data.shape <- list()\n",
    "for (n in ns) {\n",
    "    gt <- read.table(paste(n, '.s', sep=''))[,2]\n",
    "    data.seq[[n]] <- cbind(features.seq[[n]], gt)\n",
    "    data.shape[[n]] <- cbind(features.shape[[n]], gt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train model\n",
    "The caret package was used to train various models. First, the same model used in previous study e-SVR `svmLinear` was trained on sequence data as well as sequence + shape data as control. Then, the other model `glmnet` was trained on sequence + shape data to compare with the control model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data\n",
    "Split the data for training & testing for both sequence data and shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train.data.shape <- list()\n",
    "test.data.shape <- list()\n",
    "\n",
    "for (n in ns) {\n",
    "    trainIndex <- createDataPartition(data.shape[[n]][,ncol(data.shape[[n]])], p=.8, list = FALSE, times=1)\n",
    "    train.data.shape[[n]] <- data.shape[[n]][trainIndex,]\n",
    "    test.data.shape[[n]] <- data.shape[[n]][-trainIndex,]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train.data.seq <- list()\n",
    "test.data.seq <- list()\n",
    "for (n in ns) {\n",
    "    trainIndex <- createDataPartition(data.seq[[n]][,ncol(data.seq[[n]])], p=.8, list = FALSE, times=1)\n",
    "    train.data.seq[[n]] <- data.seq[[n]][trainIndex,]\n",
    "    test.data.seq[[n]] <- data.seq[[n]][-trainIndex,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set cross-validation\n",
    "Use 10-fold cross-validation in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "fitControl <- trainControl(method=\"cv\", number=10, savePredictions=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Lasso regression model (L1-normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SHAPE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.shape <- list()\n",
    "for (n in ns) {\n",
    "    model.l1.shape[[n]] <- train(gt~., data=data.frame(train.data.shape[[n]]), trControl=fitControl, method=\"glmnet\",\n",
    "                                tuneGrid=data.frame(alpha = c(2^c(-15:0)), lambda=0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n in ns) {\n",
    "    saveRDS(model.l1.shape[[n]], paste('l1_shape_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.shape <- list()\n",
    "for (n in ns) {\n",
    "    model.l1.shape[[n]] <- readRDS(paste('l1_shape_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SEQUENCE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.seq <- list()\n",
    "for (n in ns) {\n",
    "    model.l1.seq[[n]] <- train(gt~., data=data.frame(train.data.seq[[n]]), trControl=fitControl, method=\"glmnet\",\n",
    "                                tuneGrid=data.frame(alpha = c(2^c(-15:0)), lambda=0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n in ns) {\n",
    "    saveRDS(model.l1.seq[[n]], paste('l1_seq_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.seq <- list()\n",
    "for (n in ns) {\n",
    "    model.l1.seq[[n]] <- readRDS(paste('l1_seq_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Ridge regression model (L2-normalization)\n",
    "*SHAPE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\"Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\"Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    }
   ],
   "source": [
    "model.l2.shape <- list()\n",
    "for (n in ns) {\n",
    "    model.l2.shape[[n]] <- train(gt~., data=data.frame(train.data.shape[[n]]), trControl=fitControl, method=\"glmnet\",\n",
    "                   tuneGrid=data.frame(alpha = 0, lambda=c(2^c(-15:15))))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n in ns) {\n",
    "    saveRDS(model.l2.shape[[n]], paste('l2_shape_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l2.shape <- list()\n",
    "for (n in ns) {\n",
    "    model.l2.shape[[n]] <- readRDS(paste('l2_shape_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SEQUENCE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\"Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\"Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    }
   ],
   "source": [
    "model.l2.seq <- list()\n",
    "for (n in ns) {\n",
    "    model.l2.seq[[n]] <- train(gt~., data=data.frame(train.data.seq[[n]]), trControl=fitControl, method=\"glmnet\",\n",
    "                   tuneGrid=data.frame(alpha = 0, lambda=c(2^c(-15:15))))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n in ns) {\n",
    "    saveRDS(model.l2.seq[[n]], paste('l2_seq_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l2.seq <- list()\n",
    "for (n in ns) {\n",
    "    model.l2.seq[[n]] <- readRDS(paste('l2_seq_', n, '.rds', sep=''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Mad\"\n",
      "glmnet \n",
      "\n",
      "6029 samples\n",
      " 274 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 5426, 5425, 5426, 5425, 5427, 5427, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  alpha         RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3023956  0.8630964  0.2470891\n",
      "  6.103516e-05  0.3024041  0.8630885  0.2470949\n",
      "  1.220703e-04  0.3024412  0.8630538  0.2471225\n",
      "  2.441406e-04  0.3024301  0.8630736  0.2471049\n",
      "  4.882812e-04  0.3024666  0.8630537  0.2471123\n",
      "  9.765625e-04  0.3026568  0.8628752  0.2472834\n",
      "  1.953125e-03  0.2653467  0.8945058  0.2150875\n",
      "  3.906250e-03  0.2290208  0.9211907  0.1834201\n",
      "  7.812500e-03  0.2028127  0.9376016  0.1605457\n",
      "  1.562500e-02  0.1885126  0.9455057  0.1480623\n",
      "  3.125000e-02  0.1817585  0.9490030  0.1422470\n",
      "  6.250000e-02  0.1779598  0.9509932  0.1390717\n",
      "  1.250000e-01  0.1756105  0.9522567  0.1369009\n",
      "  2.500000e-01  0.1730768  0.9536027  0.1345672\n",
      "  5.000000e-01  0.1717666  0.9543011  0.1333699\n",
      "  1.000000e+00  0.1703139  0.9550688  0.1321969\n",
      "\n",
      "Tuning parameter 'lambda' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 1 and lambda = 0.\n",
      "glmnet \n",
      "\n",
      "6029 samples\n",
      " 144 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 5425, 5426, 5428, 5427, 5427, 5426, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  alpha         RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3816510  0.7743713  0.3011291\n",
      "  6.103516e-05  0.3816520  0.7743709  0.3011300\n",
      "  1.220703e-04  0.3816528  0.7743720  0.3011351\n",
      "  2.441406e-04  0.3816521  0.7743736  0.3011360\n",
      "  4.882812e-04  0.3816514  0.7743768  0.3011391\n",
      "  9.765625e-04  0.3816577  0.7743814  0.3011671\n",
      "  1.953125e-03  0.3810093  0.7744895  0.2985918\n",
      "  3.906250e-03  0.3808549  0.7745224  0.2973196\n",
      "  7.812500e-03  0.3808315  0.7745321  0.2968459\n",
      "  1.562500e-02  0.3808216  0.7745440  0.2968260\n",
      "  3.125000e-02  0.3808028  0.7745667  0.2968138\n",
      "  6.250000e-02  0.3807756  0.7745994  0.2967579\n",
      "  1.250000e-01  0.3807496  0.7746299  0.2966263\n",
      "  2.500000e-01  0.3807282  0.7746513  0.2965062\n",
      "  5.000000e-01  0.3807192  0.7746676  0.2964008\n",
      "  1.000000e+00  0.3807154  0.7746705  0.2963084\n",
      "\n",
      "Tuning parameter 'lambda' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 1 and lambda = 0.\n",
      "glmnet \n",
      "\n",
      "6029 samples\n",
      " 274 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 5427, 5425, 5426, 5427, 5426, 5426, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  lambda        RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3033207  0.8620507  0.2477532\n",
      "  6.103516e-05  0.3033207  0.8620507  0.2477532\n",
      "  1.220703e-04  0.3033207  0.8620507  0.2477532\n",
      "  2.441406e-04  0.3033207  0.8620507  0.2477532\n",
      "  4.882812e-04  0.3033207  0.8620507  0.2477532\n",
      "  9.765625e-04  0.3033207  0.8620507  0.2477532\n",
      "  1.953125e-03  0.3033207  0.8620507  0.2477532\n",
      "  3.906250e-03  0.3033207  0.8620507  0.2477532\n",
      "  7.812500e-03  0.3033207  0.8620507  0.2477532\n",
      "  1.562500e-02  0.3033207  0.8620507  0.2477532\n",
      "  3.125000e-02  0.3033207  0.8620507  0.2477532\n",
      "  6.250000e-02  0.3176470  0.8488685  0.2601328\n",
      "  1.250000e-01  0.3521634  0.8159667  0.2897698\n",
      "  2.500000e-01  0.3849640  0.7860343  0.3180013\n",
      "  5.000000e-01  0.4219920  0.7585383  0.3485765\n",
      "  1.000000e+00  0.4722785  0.7284850  0.3862560\n",
      "  2.000000e+00  0.5389013  0.6878131  0.4328848\n",
      "  4.000000e+00  0.6118084  0.6323848  0.4888483\n",
      "  8.000000e+00  0.6770447  0.5688812  0.5446746\n",
      "  1.600000e+01  0.7263383  0.5129366  0.5870148\n",
      "  3.200000e+01  0.7594845  0.4729415  0.6151989\n",
      "  6.400000e+01  0.7795026  0.4488306  0.6322360\n",
      "  1.280000e+02  0.7907855  0.4353954  0.6418723\n",
      "  2.560000e+02  0.7967928  0.4283335  0.6470278\n",
      "  5.120000e+02  0.8030951        NaN  0.6524449\n",
      "  1.024000e+03  0.8030951        NaN  0.6524449\n",
      "  2.048000e+03  0.8030951        NaN  0.6524449\n",
      "  4.096000e+03  0.8030951        NaN  0.6524449\n",
      "  8.192000e+03  0.8030951        NaN  0.6524449\n",
      "  1.638400e+04  0.8030951        NaN  0.6524449\n",
      "  3.276800e+04  0.8030951        NaN  0.6524449\n",
      "\n",
      "Tuning parameter 'alpha' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 0 and lambda = 0.03125.\n",
      "glmnet \n",
      "\n",
      "6029 samples\n",
      " 144 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 5427, 5426, 5427, 5427, 5425, 5428, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  lambda        RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3819096  0.7736038  0.3015350\n",
      "  6.103516e-05  0.3819096  0.7736038  0.3015350\n",
      "  1.220703e-04  0.3819096  0.7736038  0.3015350\n",
      "  2.441406e-04  0.3819096  0.7736038  0.3015350\n",
      "  4.882812e-04  0.3819096  0.7736038  0.3015350\n",
      "  9.765625e-04  0.3819096  0.7736038  0.3015350\n",
      "  1.953125e-03  0.3819096  0.7736038  0.3015350\n",
      "  3.906250e-03  0.3819096  0.7736038  0.3015350\n",
      "  7.812500e-03  0.3819096  0.7736038  0.3015350\n",
      "  1.562500e-02  0.3819096  0.7736038  0.3015350\n",
      "  3.125000e-02  0.3821180  0.7735680  0.3021461\n",
      "  6.250000e-02  0.3852408  0.7730656  0.3084225\n",
      "  1.250000e-01  0.3955924  0.7714085  0.3217552\n",
      "  2.500000e-01  0.4232313  0.7664449  0.3471763\n",
      "  5.000000e-01  0.4781303  0.7538844  0.3896258\n",
      "  1.000000e+00  0.5553387  0.7291784  0.4473670\n",
      "  2.000000e+00  0.6350764  0.6924930  0.5117828\n",
      "  4.000000e+00  0.6994626  0.6518795  0.5663359\n",
      "  8.000000e+00  0.7433027  0.6169480  0.6030612\n",
      "  1.600000e+01  0.7697756  0.5924517  0.6250921\n",
      "  3.200000e+01  0.7845237  0.5776299  0.6374087\n",
      "  6.400000e+01  0.7923748  0.5693564  0.6439652\n",
      "  1.280000e+02  0.7964118  0.5650221  0.6473395\n",
      "  2.560000e+02  0.7984969  0.5627730  0.6490854\n",
      "  5.120000e+02  0.8005500        NaN  0.6508052\n",
      "  1.024000e+03  0.8005500        NaN  0.6508052\n",
      "  2.048000e+03  0.8005500        NaN  0.6508052\n",
      "  4.096000e+03  0.8005500        NaN  0.6508052\n",
      "  8.192000e+03  0.8005500        NaN  0.6508052\n",
      "  1.638400e+04  0.8005500        NaN  0.6508052\n",
      "  3.276800e+04  0.8005500        NaN  0.6508052\n",
      "\n",
      "Tuning parameter 'alpha' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 0 and lambda = 0.015625.\n",
      "[1] \"Max\"\n",
      "glmnet \n",
      "\n",
      "6856 samples\n",
      " 274 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 6171, 6172, 6169, 6170, 6170, 6170, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  alpha         RMSE        Rsquared   MAE       \n",
      "  3.051758e-05  0.08018043  0.8621929  0.06440064\n",
      "  6.103516e-05  0.08018431  0.8621806  0.06440412\n",
      "  1.220703e-04  0.08018721  0.8621695  0.06440621\n",
      "  2.441406e-04  0.08019425  0.8621528  0.06441261\n",
      "  4.882812e-04  0.08015277  0.8623119  0.06437330\n",
      "  9.765625e-04  0.08019172  0.8621822  0.06440815\n",
      "  1.953125e-03  0.07363237  0.8832738  0.05875438\n",
      "  3.906250e-03  0.06740667  0.9018410  0.05339496\n",
      "  7.812500e-03  0.06269548  0.9147323  0.04931891\n",
      "  1.562500e-02  0.05976984  0.9222057  0.04679310\n",
      "  3.125000e-02  0.05813202  0.9262281  0.04537362\n",
      "  6.250000e-02  0.05721161  0.9284640  0.04455771\n",
      "  1.250000e-01  0.05667507  0.9297667  0.04405043\n",
      "  2.500000e-01  0.05632388  0.9306259  0.04374466\n",
      "  5.000000e-01  0.05610320  0.9311639  0.04354458\n",
      "  1.000000e+00  0.05597266  0.9314788  0.04343791\n",
      "\n",
      "Tuning parameter 'lambda' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 1 and lambda = 0.\n",
      "glmnet \n",
      "\n",
      "6856 samples\n",
      " 144 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 6171, 6172, 6170, 6171, 6170, 6171, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  alpha         RMSE        Rsquared   MAE       \n",
      "  3.051758e-05  0.09959079  0.7857358  0.07870994\n",
      "  6.103516e-05  0.09959072  0.7857364  0.07870996\n",
      "  1.220703e-04  0.09959063  0.7857374  0.07870993\n",
      "  2.441406e-04  0.09959042  0.7857395  0.07871001\n",
      "  4.882812e-04  0.09959125  0.7857425  0.07871217\n",
      "  9.765625e-04  0.09959049  0.7857503  0.07871243\n",
      "  1.953125e-03  0.09940871  0.7858881  0.07824621\n",
      "  3.906250e-03  0.09936152  0.7859367  0.07803048\n",
      "  7.812500e-03  0.09935247  0.7859533  0.07795809\n",
      "  1.562500e-02  0.09935087  0.7859677  0.07795517\n",
      "  3.125000e-02  0.09934294  0.7859945  0.07794373\n",
      "  6.250000e-02  0.09933276  0.7860389  0.07792844\n",
      "  1.250000e-01  0.09932235  0.7860827  0.07789952\n",
      "  2.500000e-01  0.09931441  0.7861171  0.07787320\n",
      "  5.000000e-01  0.09931006  0.7861383  0.07785329\n",
      "  1.000000e+00  0.09930796  0.7861440  0.07783435\n",
      "\n",
      "Tuning parameter 'lambda' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 1 and lambda = 0.\n",
      "glmnet \n",
      "\n",
      "6856 samples\n",
      " 274 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 6170, 6170, 6170, 6170, 6171, 6169, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  lambda        RMSE        Rsquared   MAE       \n",
      "  3.051758e-05  0.08026808  0.8621800  0.06444660\n",
      "  6.103516e-05  0.08026808  0.8621800  0.06444660\n",
      "  1.220703e-04  0.08026808  0.8621800  0.06444660\n",
      "  2.441406e-04  0.08026808  0.8621800  0.06444660\n",
      "  4.882812e-04  0.08026808  0.8621800  0.06444660\n",
      "  9.765625e-04  0.08026808  0.8621800  0.06444660\n",
      "  1.953125e-03  0.08026808  0.8621800  0.06444660\n",
      "  3.906250e-03  0.08026808  0.8621800  0.06444660\n",
      "  7.812500e-03  0.08026808  0.8621800  0.06444660\n",
      "  1.562500e-02  0.08334297  0.8519996  0.06713182\n",
      "  3.125000e-02  0.09027396  0.8290646  0.07324990\n",
      "  6.250000e-02  0.09805683  0.8050625  0.08001905\n",
      "  1.250000e-01  0.10806502  0.7786555  0.08827211\n",
      "  2.500000e-01  0.12210548  0.7451899  0.09929094\n",
      "  5.000000e-01  0.14039803  0.6965255  0.11345678\n",
      "  1.000000e+00  0.16013485  0.6290368  0.12903887\n",
      "  2.000000e+00  0.17765903  0.5538571  0.14336757\n",
      "  4.000000e+00  0.19123847  0.4886737  0.15445359\n",
      "  8.000000e+00  0.20060762  0.4438033  0.16208256\n",
      "  1.600000e+01  0.20651778  0.4167930  0.16693106\n",
      "  3.200000e+01  0.20991887  0.4020833  0.16974612\n",
      "  6.400000e+01  0.21177012  0.3943154  0.17128835\n",
      "  1.280000e+02  0.21373432        NaN  0.17293210\n",
      "  2.560000e+02  0.21373432        NaN  0.17293210\n",
      "  5.120000e+02  0.21373432        NaN  0.17293210\n",
      "  1.024000e+03  0.21373432        NaN  0.17293210\n",
      "  2.048000e+03  0.21373432        NaN  0.17293210\n",
      "  4.096000e+03  0.21373432        NaN  0.17293210\n",
      "  8.192000e+03  0.21373432        NaN  0.17293210\n",
      "  1.638400e+04  0.21373432        NaN  0.17293210\n",
      "  3.276800e+04  0.21373432        NaN  0.17293210\n",
      "\n",
      "Tuning parameter 'alpha' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 0 and lambda = 0.0078125.\n",
      "glmnet \n",
      "\n",
      "6856 samples\n",
      " 144 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 6171, 6170, 6171, 6171, 6170, 6169, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  lambda        RMSE        Rsquared   MAE       \n",
      "  3.051758e-05  0.09947483  0.7856010  0.07859864\n",
      "  6.103516e-05  0.09947483  0.7856010  0.07859864\n",
      "  1.220703e-04  0.09947483  0.7856010  0.07859864\n",
      "  2.441406e-04  0.09947483  0.7856010  0.07859864\n",
      "  4.882812e-04  0.09947483  0.7856010  0.07859864\n",
      "  9.765625e-04  0.09947483  0.7856010  0.07859864\n",
      "  1.953125e-03  0.09947483  0.7856010  0.07859864\n",
      "  3.906250e-03  0.09947483  0.7856010  0.07859864\n",
      "  7.812500e-03  0.09948249  0.7855962  0.07861344\n",
      "  1.562500e-02  0.10019086  0.7851733  0.07976361\n",
      "  3.125000e-02  0.10258291  0.7837015  0.08252494\n",
      "  6.250000e-02  0.10918540  0.7790619  0.08868158\n",
      "  1.250000e-01  0.12293494  0.7665210  0.10016857\n",
      "  2.500000e-01  0.14327011  0.7395211  0.11615522\n",
      "  5.000000e-01  0.16520070  0.6948583  0.13347881\n",
      "  1.000000e+00  0.18355627  0.6401120  0.14832253\n",
      "  2.000000e+00  0.19647979  0.5895791  0.15880107\n",
      "  4.000000e+00  0.20453579  0.5528733  0.16533169\n",
      "  8.000000e+00  0.20914409  0.5303314  0.16907747\n",
      "  1.600000e+01  0.21164155  0.5176908  0.17110605\n",
      "  3.200000e+01  0.21293995  0.5110550  0.17216257\n",
      "  6.400000e+01  0.21360676  0.5076071  0.17270540\n",
      "  1.280000e+02  0.21428148        NaN  0.17325435\n",
      "  2.560000e+02  0.21428148        NaN  0.17325435\n",
      "  5.120000e+02  0.21428148        NaN  0.17325435\n",
      "  1.024000e+03  0.21428148        NaN  0.17325435\n",
      "  2.048000e+03  0.21428148        NaN  0.17325435\n",
      "  4.096000e+03  0.21428148        NaN  0.17325435\n",
      "  8.192000e+03  0.21428148        NaN  0.17325435\n",
      "  1.638400e+04  0.21428148        NaN  0.17325435\n",
      "  3.276800e+04  0.21428148        NaN  0.17325435\n",
      "\n",
      "Tuning parameter 'alpha' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 0 and lambda = 0.00390625.\n",
      "[1] \"Myc\"\n",
      "glmnet \n",
      "\n",
      "5542 samples\n",
      " 274 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 4987, 4989, 4989, 4987, 4988, 4988, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  alpha         RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3042737  0.8557858  0.2439857\n",
      "  6.103516e-05  0.3042784  0.8557822  0.2439903\n",
      "  1.220703e-04  0.3042919  0.8557706  0.2440018\n",
      "  2.441406e-04  0.3043049  0.8557610  0.2440137\n",
      "  4.882812e-04  0.3043105  0.8557660  0.2440112\n",
      "  9.765625e-04  0.3043246  0.8557524  0.2440439\n",
      "  1.953125e-03  0.2817788  0.8759208  0.2233077\n",
      "  3.906250e-03  0.2611847  0.8930904  0.2038586\n",
      "  7.812500e-03  0.2461000  0.9047681  0.1896355\n",
      "  1.562500e-02  0.2368556  0.9115282  0.1810040\n",
      "  3.125000e-02  0.2317879  0.9151186  0.1761553\n",
      "  6.250000e-02  0.2293202  0.9168435  0.1736844\n",
      "  1.250000e-01  0.2280624  0.9177174  0.1724222\n",
      "  2.500000e-01  0.2274025  0.9181763  0.1717147\n",
      "  5.000000e-01  0.2269551  0.9184926  0.1711818\n",
      "  1.000000e+00  0.2266719  0.9186867  0.1708802\n",
      "\n",
      "Tuning parameter 'lambda' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 1 and lambda = 0.\n",
      "glmnet \n",
      "\n",
      "5542 samples\n",
      " 144 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 4988, 4987, 4989, 4989, 4987, 4987, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  alpha         RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3750247  0.7763125  0.3021094\n",
      "  6.103516e-05  0.3750241  0.7763135  0.3021088\n",
      "  1.220703e-04  0.3750244  0.7763138  0.3021105\n",
      "  2.441406e-04  0.3750239  0.7763153  0.3021107\n",
      "  4.882812e-04  0.3750259  0.7763175  0.3021168\n",
      "  9.765625e-04  0.3750272  0.7763224  0.3021258\n",
      "  1.953125e-03  0.3745089  0.7764490  0.3009223\n",
      "  3.906250e-03  0.3743856  0.7764944  0.3003829\n",
      "  7.812500e-03  0.3743664  0.7765103  0.3002212\n",
      "  1.562500e-02  0.3743547  0.7765246  0.3002075\n",
      "  3.125000e-02  0.3743340  0.7765486  0.3001922\n",
      "  6.250000e-02  0.3743020  0.7765899  0.3001334\n",
      "  1.250000e-01  0.3742688  0.7766281  0.3000700\n",
      "  2.500000e-01  0.3742446  0.7766589  0.3000069\n",
      "  5.000000e-01  0.3742213  0.7766870  0.2999621\n",
      "  1.000000e+00  0.3742190  0.7766939  0.2999225\n",
      "\n",
      "Tuning parameter 'lambda' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 1 and lambda = 0.\n",
      "glmnet \n",
      "\n",
      "5542 samples\n",
      " 274 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 4987, 4987, 4990, 4988, 4988, 4988, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  lambda        RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3038446  0.8559233  0.2435376\n",
      "  6.103516e-05  0.3038446  0.8559233  0.2435376\n",
      "  1.220703e-04  0.3038446  0.8559233  0.2435376\n",
      "  2.441406e-04  0.3038446  0.8559233  0.2435376\n",
      "  4.882812e-04  0.3038446  0.8559233  0.2435376\n",
      "  9.765625e-04  0.3038446  0.8559233  0.2435376\n",
      "  1.953125e-03  0.3038446  0.8559233  0.2435376\n",
      "  3.906250e-03  0.3038446  0.8559233  0.2435376\n",
      "  7.812500e-03  0.3038446  0.8559233  0.2435376\n",
      "  1.562500e-02  0.3038446  0.8559233  0.2435376\n",
      "  3.125000e-02  0.3038446  0.8559233  0.2435376\n",
      "  6.250000e-02  0.3234026  0.8377991  0.2613669\n",
      "  1.250000e-01  0.3471619  0.8163497  0.2828091\n",
      "  2.500000e-01  0.3744393  0.7950631  0.3065537\n",
      "  5.000000e-01  0.4122640  0.7715741  0.3373020\n",
      "  1.000000e+00  0.4680433  0.7398847  0.3803460\n",
      "  2.000000e+00  0.5395596  0.6930395  0.4365238\n",
      "  4.000000e+00  0.6137329  0.6299642  0.4988481\n",
      "  8.000000e+00  0.6768569  0.5619889  0.5505421\n",
      "  1.600000e+01  0.7233691  0.5040281  0.5869163\n",
      "  3.200000e+01  0.7539014  0.4636634  0.6102812\n",
      "  6.400000e+01  0.7721505  0.4393664  0.6242202\n",
      "  1.280000e+02  0.7823375  0.4258851  0.6320148\n",
      "  2.560000e+02  0.7877295  0.4188204  0.6361415\n",
      "  5.120000e+02  0.7933743        NaN  0.6404570\n",
      "  1.024000e+03  0.7933743        NaN  0.6404570\n",
      "  2.048000e+03  0.7933743        NaN  0.6404570\n",
      "  4.096000e+03  0.7933743        NaN  0.6404570\n",
      "  8.192000e+03  0.7933743        NaN  0.6404570\n",
      "  1.638400e+04  0.7933743        NaN  0.6404570\n",
      "  3.276800e+04  0.7933743        NaN  0.6404570\n",
      "\n",
      "Tuning parameter 'alpha' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 0 and lambda = 0.03125.\n",
      "glmnet \n",
      "\n",
      "5542 samples\n",
      " 144 predictor\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold) \n",
      "Summary of sample sizes: 4988, 4987, 4989, 4986, 4987, 4987, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  lambda        RMSE       Rsquared   MAE      \n",
      "  3.051758e-05  0.3739445  0.7782072  0.3012268\n",
      "  6.103516e-05  0.3739445  0.7782072  0.3012268\n",
      "  1.220703e-04  0.3739445  0.7782072  0.3012268\n",
      "  2.441406e-04  0.3739445  0.7782072  0.3012268\n",
      "  4.882812e-04  0.3739445  0.7782072  0.3012268\n",
      "  9.765625e-04  0.3739445  0.7782072  0.3012268\n",
      "  1.953125e-03  0.3739445  0.7782072  0.3012268\n",
      "  3.906250e-03  0.3739445  0.7782072  0.3012268\n",
      "  7.812500e-03  0.3739445  0.7782072  0.3012268\n",
      "  1.562500e-02  0.3739445  0.7782072  0.3012268\n",
      "  3.125000e-02  0.3743226  0.7781308  0.3018945\n",
      "  6.250000e-02  0.3774205  0.7775485  0.3059172\n",
      "  1.250000e-01  0.3876163  0.7756008  0.3157938\n",
      "  2.500000e-01  0.4146438  0.7698160  0.3382495\n",
      "  5.000000e-01  0.4681111  0.7556549  0.3800667\n",
      "  1.000000e+00  0.5436573  0.7293204  0.4400477\n",
      "  2.000000e+00  0.6227178  0.6931576  0.5061960\n",
      "  4.000000e+00  0.6876124  0.6563934  0.5591598\n",
      "  8.000000e+00  0.7325835  0.6264963  0.5945205\n",
      "  1.600000e+01  0.7598923  0.6063193  0.6155858\n",
      "  3.200000e+01  0.7752202  0.5941925  0.6272635\n",
      "  6.400000e+01  0.7833639  0.5875012  0.6334350\n",
      "  1.280000e+02  0.7875694  0.5839758  0.6366056\n",
      "  2.560000e+02  0.7918721        NaN  0.6398406\n",
      "  5.120000e+02  0.7918721        NaN  0.6398406\n",
      "  1.024000e+03  0.7918721        NaN  0.6398406\n",
      "  2.048000e+03  0.7918721        NaN  0.6398406\n",
      "  4.096000e+03  0.7918721        NaN  0.6398406\n",
      "  8.192000e+03  0.7918721        NaN  0.6398406\n",
      "  1.638400e+04  0.7918721        NaN  0.6398406\n",
      "  3.276800e+04  0.7918721        NaN  0.6398406\n",
      "\n",
      "Tuning parameter 'alpha' was held constant at a value of 0\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final values used for the model were alpha = 0 and lambda = 0.015625.\n"
     ]
    }
   ],
   "source": [
    "for (n in ns) {\n",
    "    print(n)\n",
    "    print(model.l1.shape[[n]])\n",
    "    print(model.l1.seq[[n]])\n",
    "    print(model.l2.shape[[n]])\n",
    "    print(model.l2.seq[[n]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 23\n",
      " $ method      : chr \"glmnet\"\n",
      " $ modelInfo   :List of 15\n",
      "  ..$ label     : chr \"glmnet\"\n",
      "  ..$ library   : chr [1:2] \"glmnet\" \"Matrix\"\n",
      "  ..$ type      : chr [1:2] \"Regression\" \"Classification\"\n",
      "  ..$ parameters:'data.frame':\t2 obs. of  3 variables:\n",
      "  .. ..$ parameter: chr [1:2] \"alpha\" \"lambda\"\n",
      "  .. ..$ class    : chr [1:2] \"numeric\" \"numeric\"\n",
      "  .. ..$ label    : chr [1:2] \"Mixing Percentage\" \"Regularization Parameter\"\n",
      "  ..$ grid      :function (x, y, len = NULL, search = \"grid\")  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 7 26 27 19 26 19 7 27\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ loop      :function (grid)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 28 26 39 19 26 19 28 39\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ fit       :function (x, y, wts, param, lev, last, classProbs, ...)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 40 25 66 19 25 19 40 66\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ predict   :function (modelFit, newdata, submodels = NULL)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 67 29 88 19 29 19 67 88\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ prob      :function (modelFit, newdata, submodels = NULL)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 89 26 122 19 26 19 89 122\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ predictors:function (x, lambda = NULL, ...)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 123 32 139 19 32 19 123 139\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ varImp    :function (object, lambda = NULL, ...)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 140 28 154 19 28 19 140 154\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ levels    :function (x)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 155 28 155 93 28 93 155 155\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ tags      : chr [1:6] \"Generalized Linear Model\" \"Implicit Feature Selection\" \"L1 Regularization\" \"L2 Regularization\" ...\n",
      "  ..$ sort      :function (x)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 159 26 159 66 26 66 159 159\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      "  ..$ trim      :function (x)  \n",
      "  .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 160 26 165 19 26 19 160 165\n",
      "  .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x000000003e936938> \n",
      " $ modelType   : chr \"Regression\"\n",
      " $ results     :'data.frame':\t16 obs. of  8 variables:\n",
      "  ..$ alpha     : num [1:16] 3.05e-05 6.10e-05 1.22e-04 2.44e-04 4.88e-04 ...\n",
      "  ..$ lambda    : num [1:16] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ RMSE      : num [1:16] 0.302 0.302 0.302 0.302 0.302 ...\n",
      "  ..$ Rsquared  : num [1:16] 0.863 0.863 0.863 0.863 0.863 ...\n",
      "  ..$ MAE       : num [1:16] 0.247 0.247 0.247 0.247 0.247 ...\n",
      "  ..$ RMSESD    : num [1:16] 0.00857 0.00857 0.00857 0.00855 0.00855 ...\n",
      "  ..$ RsquaredSD: num [1:16] 0.00984 0.00984 0.00984 0.00982 0.00983 ...\n",
      "  ..$ MAESD     : num [1:16] 0.00627 0.00627 0.00627 0.00624 0.00624 ...\n",
      " $ pred        :'data.frame':\t96464 obs. of  6 variables:\n",
      "  ..$ pred    : num [1:96464] 8.89 8.33 10.02 10.22 7.71 ...\n",
      "  ..$ obs     : num [1:96464] 8.72 8.3 10.19 10.44 8.02 ...\n",
      "  ..$ rowIndex: int [1:96464] 15 25 26 28 36 40 42 48 49 59 ...\n",
      "  ..$ alpha   : num [1:96464] 3.05e-05 3.05e-05 3.05e-05 3.05e-05 3.05e-05 ...\n",
      "  ..$ lambda  : num [1:96464] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ Resample: chr [1:96464] \"Fold01\" \"Fold01\" \"Fold01\" \"Fold01\" ...\n",
      " $ bestTune    :'data.frame':\t1 obs. of  2 variables:\n",
      "  ..$ alpha : num 1\n",
      "  ..$ lambda: num 0\n",
      " $ call        : language train.formula(form = gt ~ ., data = data.frame(train.data.shape[[n]]),      trControl = fitControl, method = \"glm| __truncated__ ...\n",
      " $ dots        : list()\n",
      " $ metric      : chr \"RMSE\"\n",
      " $ control     :List of 28\n",
      "  ..$ method           : chr \"cv\"\n",
      "  ..$ number           : num 10\n",
      "  ..$ repeats          : logi NA\n",
      "  ..$ search           : chr \"grid\"\n",
      "  ..$ p                : num 0.75\n",
      "  ..$ initialWindow    : NULL\n",
      "  ..$ horizon          : num 1\n",
      "  ..$ fixedWindow      : logi TRUE\n",
      "  ..$ skip             : num 0\n",
      "  ..$ verboseIter      : logi FALSE\n",
      "  ..$ returnData       : logi TRUE\n",
      "  ..$ returnResamp     : chr \"final\"\n",
      "  ..$ savePredictions  : chr \"all\"\n",
      "  ..$ classProbs       : logi FALSE\n",
      "  ..$ summaryFunction  :function (data, lev = NULL, model = NULL)  \n",
      "  ..$ selectionFunction: chr \"best\"\n",
      "  ..$ preProcOptions   :List of 6\n",
      "  .. ..$ thresh   : num 0.95\n",
      "  .. ..$ ICAcomp  : num 3\n",
      "  .. ..$ k        : num 5\n",
      "  .. ..$ freqCut  : num 19\n",
      "  .. ..$ uniqueCut: num 10\n",
      "  .. ..$ cutoff   : num 0.9\n",
      "  ..$ sampling         : NULL\n",
      "  ..$ index            :List of 10\n",
      "  .. ..$ Fold01: int [1:5426] 1 2 3 4 5 6 7 8 9 10 ...\n",
      "  .. ..$ Fold02: int [1:5425] 1 2 3 4 5 6 7 8 9 10 ...\n",
      "  .. ..$ Fold03: int [1:5426] 1 2 3 4 5 6 7 8 9 10 ...\n",
      "  .. ..$ Fold04: int [1:5425] 2 3 5 6 7 8 10 12 13 14 ...\n",
      "  .. ..$ Fold05: int [1:5427] 1 2 3 4 5 7 8 9 10 11 ...\n",
      "  .. ..$ Fold06: int [1:5427] 1 2 3 4 6 7 8 9 10 11 ...\n",
      "  .. ..$ Fold07: int [1:5426] 1 2 3 4 5 6 7 9 10 11 ...\n",
      "  .. ..$ Fold08: int [1:5425] 1 2 4 5 6 7 8 9 10 11 ...\n",
      "  .. ..$ Fold09: int [1:5426] 1 2 3 4 5 6 7 8 9 10 ...\n",
      "  .. ..$ Fold10: int [1:5428] 1 3 4 5 6 8 9 11 12 13 ...\n",
      "  ..$ indexOut         :List of 10\n",
      "  .. ..$ Resample01: int [1:603] 15 25 26 28 36 40 42 48 49 59 ...\n",
      "  .. ..$ Resample02: int [1:604] 19 23 37 52 57 92 107 121 124 142 ...\n",
      "  .. ..$ Resample03: int [1:603] 12 31 47 55 68 70 74 80 83 86 ...\n",
      "  .. ..$ Resample04: int [1:604] 1 4 9 11 32 39 41 44 45 58 ...\n",
      "  .. ..$ Resample05: int [1:602] 6 14 18 24 34 60 63 73 78 88 ...\n",
      "  .. ..$ Resample06: int [1:602] 5 30 33 50 71 75 81 90 91 93 ...\n",
      "  .. ..$ Resample07: int [1:603] 8 13 27 53 56 76 77 84 103 113 ...\n",
      "  .. ..$ Resample08: int [1:604] 3 46 54 85 89 99 100 125 140 161 ...\n",
      "  .. ..$ Resample09: int [1:603] 16 17 20 21 29 38 43 51 69 82 ...\n",
      "  .. ..$ Resample10: int [1:601] 2 7 10 22 35 61 62 65 66 67 ...\n",
      "  ..$ indexFinal       : NULL\n",
      "  ..$ timingSamps      : num 0\n",
      "  ..$ predictionBounds : logi [1:2] FALSE FALSE\n",
      "  ..$ seeds            :List of 11\n",
      "  .. ..$ : int [1:16] 530831 471713 952328 569834 519332 832622 854987 439374 339943 695606 ...\n",
      "  .. ..$ : int [1:16] 486827 486366 750974 567888 999749 453796 523580 313283 109320 49147 ...\n",
      "  .. ..$ : int [1:16] 50450 849917 784640 706828 950733 593349 483028 160065 528884 572077 ...\n",
      "  .. ..$ : int [1:16] 332777 907151 598574 330633 727817 804329 66625 253255 124342 76178 ...\n",
      "  .. ..$ : int [1:16] 866360 902444 206526 530113 612034 226250 37622 734167 346452 366816 ...\n",
      "  .. ..$ : int [1:16] 804626 382521 624218 681738 190092 286446 367994 45138 319322 18956 ...\n",
      "  .. ..$ : int [1:16] 788836 855778 992281 739908 705773 754871 793908 275960 631686 186160 ...\n",
      "  .. ..$ : int [1:16] 864133 465291 555611 19379 128513 935538 530567 443127 568190 612586 ...\n",
      "  .. ..$ : int [1:16] 33223 879750 238378 329328 705726 139210 375061 882026 327812 784660 ...\n",
      "  .. ..$ : int [1:16] 862948 828980 91905 306483 467344 834387 59561 959691 569427 943795 ...\n",
      "  .. ..$ : int 941530\n",
      "  ..$ adaptive         :List of 4\n",
      "  .. ..$ min     : num 5\n",
      "  .. ..$ alpha   : num 0.05\n",
      "  .. ..$ method  : chr \"gls\"\n",
      "  .. ..$ complete: logi TRUE\n",
      "  ..$ trim             : logi FALSE\n",
      "  ..$ allowParallel    : logi TRUE\n",
      "  ..$ yLimits          : num [1:2] 7.02 11.04\n",
      " $ finalModel  :List of 18\n",
      "  ..$ a0         : Named num [1:100] 8.65 9.78 10.83 11.79 12.66 ...\n",
      "  .. ..- attr(*, \"names\")= chr [1:100] \"s0\" \"s1\" \"s2\" \"s3\" ...\n",
      "  ..$ beta       :Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n",
      "  .. .. ..@ i       : int [1:10175] 159 160 159 160 159 160 159 160 159 160 ...\n",
      "  .. .. ..@ p       : int [1:101] 0 0 2 4 6 8 10 12 14 16 ...\n",
      "  .. .. ..@ Dim     : int [1:2] 274 100\n",
      "  .. .. ..@ Dimnames:List of 2\n",
      "  .. .. .. ..$ : chr [1:274] \"V1\" \"V2\" \"V3\" \"V4\" ...\n",
      "  .. .. .. ..$ : chr [1:100] \"s0\" \"s1\" \"s2\" \"s3\" ...\n",
      "  .. .. ..@ x       : num [1:10175] -0.097 -0.126 -0.201 -0.229 -0.295 ...\n",
      "  .. .. ..@ factors : list()\n",
      "  ..$ df         : int [1:100] 0 2 2 2 2 2 2 2 2 2 ...\n",
      "  ..$ dim        : int [1:2] 274 100\n",
      "  ..$ lambda     : num [1:100] 0.477 0.435 0.396 0.361 0.329 ...\n",
      "  ..$ dev.ratio  : num [1:100] 0 0.069 0.128 0.176 0.217 ...\n",
      "  ..$ nulldev    : num 3889\n",
      "  ..$ npasses    : int 12610\n",
      "  ..$ jerr       : int 0\n",
      "  ..$ offset     : logi FALSE\n",
      "  ..$ call       : language (function (x, y, family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\",      \"cox\", \"mgaussian\"), weights, | __truncated__ ...\n",
      "  ..$ nobs       : int 6029\n",
      "  ..$ lambdaOpt  : num 0\n",
      "  ..$ xNames     : chr [1:274] \"V1\" \"V2\" \"V3\" \"V4\" ...\n",
      "  ..$ problemType: chr \"Regression\"\n",
      "  ..$ tuneValue  :'data.frame':\t1 obs. of  2 variables:\n",
      "  .. ..$ alpha : num 1\n",
      "  .. ..$ lambda: num 0\n",
      "  ..$ obsLevels  : logi NA\n",
      "  ..$ param      : list()\n",
      "  ..- attr(*, \"class\")= chr [1:2] \"elnet\" \"glmnet\"\n",
      " $ preProcess  : NULL\n",
      " $ trainingData:'data.frame':\t6029 obs. of  275 variables:\n",
      "  ..$ .outcome: num [1:6029] 8.76 7.9 10.21 8.33 10.39 ...\n",
      "  ..$ V1      : num [1:6029] 0 0 0 1 0 0 0 1 0 1 ...\n",
      "  ..$ V2      : num [1:6029] 1 0 1 0 1 0 0 0 1 0 ...\n",
      "  ..$ V3      : num [1:6029] 0 1 0 0 0 1 1 0 0 0 ...\n",
      "  ..$ V4      : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V5      : num [1:6029] 0 0 0 0 0 0 0 0 0 1 ...\n",
      "  ..$ V6      : num [1:6029] 1 1 1 1 0 0 0 1 1 0 ...\n",
      "  ..$ V7      : num [1:6029] 0 0 0 0 1 0 0 0 0 0 ...\n",
      "  ..$ V8      : num [1:6029] 0 0 0 0 0 1 1 0 0 0 ...\n",
      "  ..$ V9      : num [1:6029] 0 0 0 0 0 0 0 0 0 1 ...\n",
      "  ..$ V10     : num [1:6029] 1 1 0 0 0 1 1 0 1 0 ...\n",
      "  ..$ V11     : num [1:6029] 0 0 1 1 0 0 0 0 0 0 ...\n",
      "  ..$ V12     : num [1:6029] 0 0 0 0 1 0 0 1 0 0 ...\n",
      "  ..$ V13     : num [1:6029] 0 0 0 1 0 0 0 0 0 0 ...\n",
      "  ..$ V14     : num [1:6029] 0 1 1 0 1 0 0 0 0 1 ...\n",
      "  ..$ V15     : num [1:6029] 1 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V16     : num [1:6029] 0 0 0 0 0 1 1 1 1 0 ...\n",
      "  ..$ V17     : num [1:6029] 0 0 0 0 0 0 1 0 0 0 ...\n",
      "  ..$ V18     : num [1:6029] 0 1 1 0 0 0 0 1 0 1 ...\n",
      "  ..$ V19     : num [1:6029] 0 0 0 1 1 1 0 0 0 0 ...\n",
      "  ..$ V20     : num [1:6029] 1 0 0 0 0 0 0 0 1 0 ...\n",
      "  ..$ V21     : num [1:6029] 1 0 0 0 0 0 0 1 1 0 ...\n",
      "  ..$ V22     : num [1:6029] 0 0 0 1 1 0 0 0 0 0 ...\n",
      "  ..$ V23     : num [1:6029] 0 0 1 0 0 1 0 0 0 0 ...\n",
      "  ..$ V24     : num [1:6029] 0 1 0 0 0 0 1 0 0 1 ...\n",
      "  ..$ V25     : num [1:6029] 0 0 0 0 0 1 0 1 1 0 ...\n",
      "  ..$ V26     : num [1:6029] 1 1 0 0 1 0 0 0 0 0 ...\n",
      "  ..$ V27     : num [1:6029] 0 0 1 1 0 0 0 0 0 0 ...\n",
      "  ..$ V28     : num [1:6029] 0 0 0 0 0 0 1 0 0 1 ...\n",
      "  ..$ V29     : num [1:6029] 0 0 0 0 0 0 0 1 0 0 ...\n",
      "  ..$ V30     : num [1:6029] 0 1 0 0 0 0 0 0 0 1 ...\n",
      "  ..$ V31     : num [1:6029] 0 0 1 1 1 1 0 0 1 0 ...\n",
      "  ..$ V32     : num [1:6029] 1 0 0 0 0 0 1 0 0 0 ...\n",
      "  ..$ V33     : num [1:6029] 0 0 0 1 0 0 1 1 1 1 ...\n",
      "  ..$ V34     : num [1:6029] 0 0 1 0 0 0 0 0 0 0 ...\n",
      "  ..$ V35     : num [1:6029] 0 0 0 0 1 0 0 0 0 0 ...\n",
      "  ..$ V36     : num [1:6029] 1 1 0 0 0 1 0 0 0 0 ...\n",
      "  ..$ V37     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V38     : num [1:6029] 0 1 1 1 0 1 0 0 0 0 ...\n",
      "  ..$ V39     : num [1:6029] 0 0 0 0 1 0 0 1 0 0 ...\n",
      "  ..$ V40     : num [1:6029] 1 0 0 0 0 0 1 0 1 1 ...\n",
      "  ..$ V41     : num [1:6029] 0 0 0 0 0 0 0 1 0 0 ...\n",
      "  ..$ V42     : num [1:6029] 1 0 1 0 0 0 0 0 0 0 ...\n",
      "  ..$ V43     : num [1:6029] 0 1 0 1 1 0 0 0 1 0 ...\n",
      "  ..$ V44     : num [1:6029] 0 0 0 0 0 1 1 0 0 1 ...\n",
      "  ..$ V45     : num [1:6029] 0 0 0 0 0 0 0 0 1 1 ...\n",
      "  ..$ V46     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V47     : num [1:6029] 1 1 1 1 0 1 0 1 0 0 ...\n",
      "  ..$ V48     : num [1:6029] 0 0 0 0 1 0 1 0 0 0 ...\n",
      "  ..$ V49     : num [1:6029] 0 0 0 1 1 0 1 0 1 1 ...\n",
      "  ..$ V50     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V51     : num [1:6029] 1 0 0 0 0 1 0 1 0 0 ...\n",
      "  ..$ V52     : num [1:6029] 0 1 1 0 0 0 0 0 0 0 ...\n",
      "  ..$ V53     : num [1:6029] 0 0 1 0 0 0 0 0 0 0 ...\n",
      "  ..$ V54     : num [1:6029] 0 0 0 1 0 0 1 0 1 1 ...\n",
      "  ..$ V55     : num [1:6029] 0 1 0 0 1 1 0 1 0 0 ...\n",
      "  ..$ V56     : num [1:6029] 1 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V57     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V58     : num [1:6029] 0 0 0 0 0 0 1 0 1 1 ...\n",
      "  ..$ V59     : num [1:6029] 1 1 1 1 1 1 0 1 0 0 ...\n",
      "  ..$ V60     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V61     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V62     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V63     : num [1:6029] 1 1 1 1 1 1 1 1 1 1 ...\n",
      "  ..$ V64     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V65     : num [1:6029] 1 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V66     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V67     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V68     : num [1:6029] 0 1 1 1 1 1 1 1 1 1 ...\n",
      "  ..$ V69     : num [1:6029] 0 0 0 1 0 0 0 0 0 1 ...\n",
      "  ..$ V70     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V71     : num [1:6029] 1 1 1 0 1 1 1 1 1 0 ...\n",
      "  ..$ V72     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V73     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V74     : num [1:6029] 1 1 1 1 1 0 0 1 1 1 ...\n",
      "  ..$ V75     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V76     : num [1:6029] 0 0 0 0 0 1 1 0 0 0 ...\n",
      "  ..$ V77     : num [1:6029] 1 0 1 1 1 1 1 0 1 1 ...\n",
      "  ..$ V78     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V79     : num [1:6029] 0 1 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V80     : num [1:6029] 0 0 0 0 0 0 0 1 0 0 ...\n",
      "  ..$ V81     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V82     : num [1:6029] 1 1 1 1 1 1 1 1 1 1 ...\n",
      "  ..$ V83     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V84     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V85     : num [1:6029] 0 0 0 0 0 0 0 0 0 1 ...\n",
      "  ..$ V86     : num [1:6029] 1 0 1 0 1 0 1 1 1 0 ...\n",
      "  ..$ V87     : num [1:6029] 0 1 0 1 0 1 0 0 0 0 ...\n",
      "  ..$ V88     : num [1:6029] 0 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V89     : num [1:6029] 1 0 0 0 1 0 1 0 0 0 ...\n",
      "  ..$ V90     : num [1:6029] 0 0 1 0 0 0 0 0 0 0 ...\n",
      "  ..$ V91     : num [1:6029] 0 1 0 0 0 0 0 1 1 1 ...\n",
      "  ..$ V92     : num [1:6029] 0 0 0 1 0 1 0 0 0 0 ...\n",
      "  ..$ V93     : num [1:6029] 1 0 1 0 1 0 0 1 0 1 ...\n",
      "  ..$ V94     : num [1:6029] 0 0 0 1 0 1 0 0 0 0 ...\n",
      "  ..$ V95     : num [1:6029] 0 0 0 0 0 0 0 0 1 0 ...\n",
      "  ..$ V96     : num [1:6029] 0 1 0 0 0 0 1 0 0 0 ...\n",
      "  ..$ V97     : num [1:6029] 1 0 0 0 0 0 0 0 0 0 ...\n",
      "  ..$ V98     : num [1:6029] 0 0 0 0 0 1 1 1 0 1 ...\n",
      "  .. [list output truncated]\n",
      " $ resample    :'data.frame':\t10 obs. of  4 variables:\n",
      "  ..$ RMSE    : num [1:10] 0.177 0.167 0.171 0.165 0.177 ...\n",
      "  ..$ Rsquared: num [1:10] 0.95 0.96 0.954 0.958 0.952 ...\n",
      "  ..$ MAE     : num [1:10] 0.135 0.133 0.131 0.128 0.139 ...\n",
      "  ..$ Resample: chr [1:10] \"Fold01\" \"Fold06\" \"Fold07\" \"Fold03\" ...\n",
      " $ resampledCM : NULL\n",
      " $ perfNames   : chr [1:3] \"RMSE\" \"Rsquared\" \"MAE\"\n",
      " $ maximize    : logi FALSE\n",
      " $ yLimits     : num [1:2] 7.02 11.04\n",
      " $ times       :List of 3\n",
      "  ..$ everything: 'proc_time' Named num [1:5] 155.2 3.5 160.7 NA NA\n",
      "  .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ...\n",
      "  ..$ final     : 'proc_time' Named num [1:5] 1.71 0 1.72 NA NA\n",
      "  .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ...\n",
      "  ..$ prediction: logi [1:3] NA NA NA\n",
      " $ levels      : logi NA\n",
      " $ terms       :Classes 'terms', 'formula'  language gt ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 +      V14 + V15 + V16 + V17 + V18 + V19 | __truncated__ ...\n",
      "  .. ..- attr(*, \"variables\")= language list(gt, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15,      V16, V17, V18, V19, V20, V21, V22| __truncated__ ...\n",
      "  .. ..- attr(*, \"factors\")= int [1:275, 1:274] 0 1 0 0 0 0 0 0 0 0 ...\n",
      "  .. .. ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. .. .. ..$ : chr [1:275] \"gt\" \"V1\" \"V2\" \"V3\" ...\n",
      "  .. .. .. ..$ : chr [1:274] \"V1\" \"V2\" \"V3\" \"V4\" ...\n",
      "  .. ..- attr(*, \"term.labels\")= chr [1:274] \"V1\" \"V2\" \"V3\" \"V4\" ...\n",
      "  .. ..- attr(*, \"order\")= int [1:274] 1 1 1 1 1 1 1 1 1 1 ...\n",
      "  .. ..- attr(*, \"intercept\")= int 1\n",
      "  .. ..- attr(*, \"response\")= int 1\n",
      "  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n",
      "  .. ..- attr(*, \"predvars\")= language list(gt, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15,      V16, V17, V18, V19, V20, V21, V22| __truncated__ ...\n",
      "  .. ..- attr(*, \"dataClasses\")= Named chr [1:275] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n",
      "  .. .. ..- attr(*, \"names\")= chr [1:275] \"gt\" \"V1\" \"V2\" \"V3\" ...\n",
      " $ coefnames   : chr [1:274] \"V1\" \"V2\" \"V3\" \"V4\" ...\n",
      " $ xlevels     : Named list()\n",
      " - attr(*, \"class\")= chr [1:2] \"train\" \"train.formula\"\n"
     ]
    }
   ],
   "source": [
    "str(model.l1.shape[[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.l1.shape <- list()\n",
    "pred.l1.seq <- list()\n",
    "for (n in ns) {\n",
    "    pred.l1.shape[[n]] <- predict(model.l1.shape[[n]], test.data.shape[[n]])\n",
    "    pred.l1.seq[[n]] <- predict(model.l1.seq[[n]], test.data.seq[[n]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.l2.shape <- list()\n",
    "pred.l2.seq <- list()\n",
    "for (n in ns) {\n",
    "    pred.l2.shape[[n]] <- predict(model.l2.shape[[n]], test.data.shape[[n]])\n",
    "    pred.l2.seq[[n]] <- predict(model.l2.seq[[n]], test.data.seq[[n]])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare sequence vs shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Theme\n",
    "my.theme <- theme(\n",
    "  axis.text = element_text(colour=\"black\", size=12),\n",
    "  axis.title.x = element_text(colour=\"black\", size=12),\n",
    "  axis.title.y = element_text(colour=\"black\", size=12),\n",
    "  panel.grid.major = element_blank(),\n",
    "  panel.grid.minor = element_blank(),\n",
    "  panel.background = element_blank(),\n",
    "  axis.line = element_line(colour = \"black\"),\n",
    "#  axis.text = element_text(colour =\"black\"),\n",
    "  axis.ticks = element_line(colour = \"black\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS <- function(y_prediction, y_testing){\n",
    "   1 - ( sum( y_testing - y_prediction ) ^ 2 ) / sum( (y_testing - mean( y_testing ) ) ^ 2 )  \n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rsquared on Test Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAzMzNNTU1oaGh8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\nAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nHELvAp9kzQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- c()\n",
    "y <- c()\n",
    "for (n in ns) {\n",
    "    x <- c(x,RS(pred.l1.shape[[n]], data.frame(test.data.shape[[n]])$gt))\n",
    "    y <- c(y,RS(pred.l1.seq[[n]], data.frame(test.data.seq[[n]])$gt))\n",
    "}\n",
    "print(ggplot() +\n",
    "  geom_point(aes(x = x, y = y), color = \"red\", size=5) +\n",
    "  geom_abline(slope=1) + geom_vline(xintercept=0) + geom_hline(yintercept=0) +\n",
    "  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +\n",
    "  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))) + my.theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAzMzNNTU1oaGh8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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- c()\n",
    "y <- c()\n",
    "for (n in ns) {\n",
    "    x <- c(x,RS(pred.l2.shape[[n]], data.frame(test.data.shape[[n]])$gt))\n",
    "    y <- c(y,RS(pred.l2.seq[[n]], data.frame(test.data.seq[[n]])$gt))\n",
    "}\n",
    "print(ggplot() +\n",
    "  geom_point(aes(x = x, y = y), color = \"red\", size=5) +\n",
    "  geom_abline(slope=1) + geom_vline(xintercept=0) + geom_hline(yintercept=0) +\n",
    "  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +\n",
    "  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))) + my.theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAzMzNNTU1oaGh8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\nxOE0OIjE4TQ4iMThNDj/D72NNDvHE117AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nCQoQEhQgJCjgfwIkQwDDBNxYAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- c()\n",
    "y <- c()\n",
    "for (n in ns) {\n",
    "    x <- c(x,RS(pred.l1.shape[[n]], data.frame(test.data.shape[[n]])$gt))\n",
    "    y <- c(y,RS(pred.l2.shape[[n]], data.frame(test.data.shape[[n]])$gt))\n",
    "}\n",
    "print(ggplot() +\n",
    "  geom_point(aes(x = x, y = y), color = \"red\", size=5) +\n",
    "  geom_abline(slope=1) + geom_vline(xintercept=0) + geom_hline(yintercept=0) +\n",
    "  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +\n",
    "  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))) + my.theme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
